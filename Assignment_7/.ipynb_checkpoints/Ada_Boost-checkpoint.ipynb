{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1. AdaBoost\n",
    "Implement AdaBoost using Python (incl. Numpy etc.) and use it on the SPAM-Dataset\n",
    ".\n",
    "The weak classifiers should be decision stumps (i.e. decision trees with one node).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the spambase dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('/Users/Eva/Downloads/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "df_names = pd.read_csv('/Users/Eva/Downloads/spambase.names', header=None, names=[\"feature_names\"], skiprows=32)\n",
    "df_names[\"feature_names\"] = df_names[\"feature_names\"].str.replace(\":  *continuous.\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450, 57), (1151, 57), (3450,), (1151,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Tree Implementation from Week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree():\n",
    "\n",
    "\n",
    "    def __init__(self, node_min_size=2, tree_max_depth=None, post_pruning=True):\n",
    "        \"\"\"\n",
    "        If no maximum depth is defined, the nodes are split while they contain at least node_min_size many points.\n",
    "        If maximum depth is defined, nodes are split further until they contain less than node_min_size many points or\n",
    "        have reached the maximum number of allowed split levels.\n",
    "        \"\"\"\n",
    "        self.depth = 1\n",
    "        self.node_min_size = node_min_size\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        self.prune = 0\n",
    "        self.post_pruning = post_pruning\n",
    "        self.feature_importances = defaultdict(int)\n",
    "        \n",
    "    ################ Tree building ################\n",
    "    \n",
    "    def split_data(self, X, j, v):\n",
    "        \"\"\"\n",
    "        Split data X at variable with index j and splitting point v.\n",
    "        \"\"\"\n",
    "        split_left = X[np.where(X[:,j]<=v)]\n",
    "        split_right = X[np.where(X[:,j]>v)]\n",
    "        return split_left, split_right\n",
    "\n",
    "    \n",
    "    def gini_index(self, groups):\n",
    "        \"\"\"\n",
    "        Calculate gini index for a data set splitted on a certain variable and splitting point.\n",
    "        groups: two subsets that result from splitting a data set\n",
    "        y: associated class labels\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        if type(groups[0]) == np.ndarray:\n",
    "            n = sum(map(len,groups))\n",
    "            # get class proportions for each split group\n",
    "            for group in groups:\n",
    "                size = float(len(group))\n",
    "                y = group[:,-1]\n",
    "                if size == 0:\n",
    "                    continue    \n",
    "                score = 0\n",
    "                # sum class proportions within one split groups as sum(p_mk*(1-p_mk))\n",
    "                for c in set(y):\n",
    "                    proportions = np.count_nonzero(y==c)/size\n",
    "                    score += proportions*(1-proportions)\n",
    "                # sum \"inversely squared\" class proportions per split group in weighted sum (weighted by proportion of group size)\n",
    "                gini += score * size/n\n",
    "        else:\n",
    "            size = len(groups)\n",
    "            for c in set(groups):\n",
    "                proportions = groups.count(c)/size\n",
    "                gini += proportions*(1-proportions)\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def split_node(self, data):\n",
    "        \"\"\"\n",
    "        Find optimal split for a given node \"node\" and class labels \"y\" that minimizes the loss function (gini index).\n",
    "        \"\"\"\n",
    "        split_var, split_point, split_gini, groups = 9999, 9999, 9999, None\n",
    "        # for every candidate split X*p* calculate gini index and select split which minimizes the score\n",
    "        for j in range(data.shape[1]-1):\n",
    "            candidate_p = np.unique(X[:,j])\n",
    "            for v in candidate_p:\n",
    "                split_groups = self.split_data(data, j, v)\n",
    "                s = self.gini_index(split_groups) \n",
    "                if s < split_gini: \n",
    "                    split_var, split_point, split_gini, groups = j, v, s, split_groups\n",
    "        return {'split_var': split_var, 'split_point': split_point, 'split_gini': split_gini, 'groups': groups}\n",
    "\n",
    "\n",
    "    def make_leaf(self, node_labels):\n",
    "        \"\"\"\n",
    "        Convert node \"node\" to leaf by assigning it the majority vote of labels \"y\".\n",
    "        \"\"\"\n",
    "        leaf_labels = list(node_labels)\n",
    "        return {'label': max(set(leaf_labels), key=leaf_labels.count), 'class_sizes': [leaf_labels.count(c) for c in set(leaf_labels)]} \n",
    "\n",
    "    \n",
    "    def grow(self, node):\n",
    "        \"\"\"\n",
    "        Recusively split nodes until stopping criteria are fulfilled,\n",
    "        i.e. a node has too little data points in its region to be splitted further or the the has\n",
    "        reached its maximum allowd depth.\n",
    "        \"\"\"\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # Check for empty nodes (pruned in post-processing)\n",
    "        if not left.any() or not right.any():\n",
    "            node['left'] = node['right'] = self.make_leaf(np.r_[left[:,-1], right[:,-1]])\n",
    "            return None\n",
    "        # Check for max depth\n",
    "        if self.tree_max_depth:\n",
    "            if self.depth >= self.tree_max_depth:\n",
    "                node['left'] = self.make_leaf(left[:,-1])\n",
    "                node['right'] = self.make_leaf(right[:,-1])\n",
    "                return None\n",
    "        # Check left node size\n",
    "        if len(left) <= self.node_min_size-1:\n",
    "            node['left'] = self.make_leaf(left[:,-1])\n",
    "        else:\n",
    "            node['left'] = self.split_node(left)\n",
    "            self.depth += 1\n",
    "            self.grow(node['left'])\n",
    "        # Check right node size\n",
    "        if len(right) <= self.node_min_size:\n",
    "            node['right'] = self.make_leaf(right[:,-1])\n",
    "        else:\n",
    "            node['right'] = self.split_node(right)\n",
    "            self.depth += 1\n",
    "            self.grow(node['right'])\n",
    "        return None\n",
    "    \n",
    "    ################ Post-Pruning ################\n",
    "    \n",
    "    def check_twin_leaves(self, subtree):\n",
    "        \"\"\"\n",
    "        Prune tree while there still are twin leaves\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():  \n",
    "            if 'label' in subtree['left'].keys() and 'label' in subtree['right'].keys():\n",
    "                if subtree['left']['label'] == subtree['right']['label']:\n",
    "                    self.prune += 1\n",
    "            else:\n",
    "                self.check_twin_leaves(subtree['left'])\n",
    "                self.check_twin_leaves(subtree['right'])\n",
    "\n",
    "    def prune_tree(self, subtree):\n",
    "        \"\"\"\n",
    "        If a node results in leaves of the same label, remove leaves and make node to leaf\n",
    "        \"\"\"\n",
    "        # check if A is node or leaf\n",
    "        if not 'label' in subtree.keys():  \n",
    "            # at least one subtree B1, B2 has to be a node\n",
    "            if not 'label' in subtree['left'].keys() or not 'label' in subtree['right'].keys():\n",
    "                # prune left subtree B1?\n",
    "                if not 'label' in subtree['left'].keys():\n",
    "                    tmp = subtree['left']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['left'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['right'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "                # prune right subtree B2?           \n",
    "                if not 'label' in subtree['right'].keys():\n",
    "                    tmp = subtree['right']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['right'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['left'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "\n",
    "    ################ Build tree from data  ################\n",
    "    \n",
    "    def grow_tree(self, X, y):\n",
    "        \"\"\"\n",
    "        Grow a decision tree for feature data X with class labels y that has at most tree_max_depth many levels\n",
    "        and at least node_mins_size many data points that support each leaf prediction.\n",
    "        \"\"\"\n",
    "        print('Build tree from data')\n",
    "        if X.shape[0] <= self.node_min_size:\n",
    "            raise Exception(\"Data set is too small to build a tree!\")\n",
    "        root = self.split_node(np.c_[X,y])\n",
    "        self.grow(root)\n",
    "        if self.post_pruning:\n",
    "            print('Post-pruning')\n",
    "            self.check_twin_leaves(root)\n",
    "            while self.prune >= 1:\n",
    "                self.prune_tree(root)\n",
    "                self.check_twin_leaves(root)\n",
    "        print('Feature importances')\n",
    "        for i in range(X.shape[1]):\n",
    "            self.get_feature_importance(root,i)\n",
    "        f = float(sum(self.feature_importances.values()))\n",
    "        for k,v in self.feature_importances.items():\n",
    "            self.feature_importances[k] = v/f\n",
    "        return root   \n",
    "            \n",
    "    ################ Classification ################\n",
    "    \n",
    "    def get_label(self, test, subtree):\n",
    "        \"\"\"\n",
    "        Process data query through the decision tree and retrieve resulting leaf label.\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if test[subtree['split_var']] <= subtree['split_point']:\n",
    "                subtree = subtree['left']\n",
    "                return self.get_label(test, subtree)\n",
    "            else:\n",
    "                subtree = subtree['right']\n",
    "                return self.get_label(test, subtree)\n",
    "        else:\n",
    "            return subtree['label']\n",
    "        \n",
    "        \n",
    "    def predict(self, tree, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for data set X.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for item in X:\n",
    "            y_pred.append(self.get_label(item, tree))\n",
    "        return y_pred\n",
    "    \n",
    "    ################ Evaluation ################\n",
    "    \n",
    "    def calculate_accuracy(self, true_y, pred_y, classes):\n",
    "        \"\"\"\n",
    "        Calculate accuracy for a classified set.\n",
    "        \"\"\"\n",
    "        class_sum = 0\n",
    "        for class_num in classes:\n",
    "            val_sum = 0\n",
    "            for true_val, pred_val in zip(true_y, pred_y): \n",
    "                if class_num == true_val:\n",
    "                    if true_val == pred_val:\n",
    "                        val_sum += 1\n",
    "            class_sum += val_sum\n",
    "        return class_sum / len(true_y)\n",
    "    \n",
    "    def get_feature_importance(self, subtree, var):\n",
    "        \"\"\"\n",
    "        This Function gets the gini loss from all subtree nodes.\n",
    "        Possible TODO in the Future: weight scores by probability to reach respective node, i.e. node size/data size\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if subtree['split_var'] == var:\n",
    "                if 'class_sizes' in subtree['left'].keys():\n",
    "                    ginil = self.gini_index(subtree['left']['class_sizes'])\n",
    "                else: \n",
    "                    ginil = subtree['left']['split_gini']\n",
    "                if 'class_sizes' in subtree['right'].keys():\n",
    "                    ginir = self.gini_index(subtree['right']['class_sizes'])\n",
    "                else:\n",
    "                    ginir = subtree['right']['split_gini']\n",
    "                self.feature_importances[var] += subtree['split_var']-(ginir+ginil)\n",
    "\n",
    "            self.get_feature_importance(subtree['left'], var)\n",
    "            self.get_feature_importance(subtree['right'], var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Ada-Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ada_Boost:\n",
    "    def __init__(self, n_predictors, tree_depth=1):\n",
    "        self.n_predictors = n_predictors\n",
    "        self.tree_depth = tree_depth\n",
    "        self.classifier_list = []\n",
    "        \n",
    "    def calculate_Em(self, sample_weights, y_train, y_pred):\n",
    "        return np.sum(sample_weights * (y_train == y_pred).astype(int)) / np.linalg.norm(sample_weights, ord=1)\n",
    "        \n",
    "    \n",
    "    def train_ada_boost(self, X_train, y_train):\n",
    "        N = X_train.shape[0]\n",
    "        #1) initialize wi\n",
    "        sample_weights = np.array([1 / N] * N)\n",
    "        X_resampled = X_train.copy()\n",
    "        #2) for m in {1,...,M}\n",
    "        for m in range(self.n_predictors):\n",
    "            # resample X_train according to the weights\n",
    "            choices = np.random.choice(np.arange(N), N, p=sample_weights)\n",
    "            X_resampled = X_resampled[choices]\n",
    "            # set the weights of the resampled set to 1/N\n",
    "            sample_weights = np.array([1 / N] * N)\n",
    "            # 2a) train a classifier on X_train with weights wi\n",
    "            classifier = ClassificationTree(tree_max_depth=self.tree_depth)\n",
    "            tree = classifier.grow_tree(X_resampled, y_train)\n",
    "            y_train_pred = classifier.predict(tree, X_resampled)\n",
    "            # 2b) compute the classification error\n",
    "            classification_error = self.calculate_Em(sample_weights, y_train, y_train_pred)\n",
    "            # 2c) compute classifier weight\n",
    "            classifier_weight = np.log((1 - classification_error) / classification_error) / 2\n",
    "            self.classifier_list.append((classifier_weight, tree, classifier))\n",
    "            # 2d) recompute sample weights\n",
    "            sample_weights = sample_weights * np.exp(-classifier_weight * y_train * y_train_pred)\n",
    "            sample_weights = sample_weights / np.linalg.norm(sample_weights, ord=1)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        # 3) return ensemble model output\n",
    "        X_m = np.array([am * np.array(classifier.predict(tree, X_test)) for am, tree, classifier in self.classifier_list])\n",
    "        return np.sign(np.sum(X_m, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the labels to -1/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_changed = np.where(y_train==0, -1, 1)\n",
    "y_test_changed = np.where(y_test==0, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Ada-Boost on the Spambase Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build tree from data\n"
     ]
    }
   ],
   "source": [
    "ada = Ada_Boost(n_predictors = 3, tree_depth = 1)\n",
    "ada.train_ada_boost(X_train, y_train_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.22335153182646397,\n",
       "  {'split_var': 34,\n",
       "   'split_point': 1.49,\n",
       "   'split_gini': 0.4758341785826028,\n",
       "   'left': {'label': -1.0, 'class_sizes': [1314, 2059]},\n",
       "   'right': {'label': 1.0, 'class_sizes': [45, 32]}},\n",
       "  <__main__.ClassificationTree at 0x2b070076608>),\n",
       " (-0.22152492231862247,\n",
       "  {'split_var': 2,\n",
       "   'split_point': 2.38,\n",
       "   'split_gini': 0.47622545981451764,\n",
       "   'left': {'label': -1.0, 'class_sizes': [1336, 2078]},\n",
       "   'right': {'label': 1.0, 'class_sizes': [23, 13]}},\n",
       "  <__main__.ClassificationTree at 0x2b06e1cb0c8>),\n",
       " (-0.2221336299823221,\n",
       "  {'split_var': 22,\n",
       "   'split_point': 1.62,\n",
       "   'split_gini': 0.47603358346867003,\n",
       "   'left': {'label': -1.0, 'class_sizes': [1338, 2081]},\n",
       "   'right': {'label': 1.0, 'class_sizes': [21, 10]}},\n",
       "  <__main__.ClassificationTree at 0x2b06dc2eec8>)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada.classifier_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-1b80c0151853>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_changed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-d691090b3a56>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# 3) return ensemble model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mX_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mam\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-d691090b3a56>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# 3) return ensemble model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mX_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mam\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-52041c0706dc>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, tree, X)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-52041c0706dc>\u001b[0m in \u001b[0;36mget_label\u001b[1;34m(self, test, subtree)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \"\"\"\n\u001b[0;32m    197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34m'label'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'split_var'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0msubtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'split_point'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                 \u001b[0msubtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubtree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "y_pred = ada.predict(y_test_changed)\n",
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Print a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_pred, y_true, class_label_list):\n",
    "    \"\"\"\n",
    "    Returns a confusion matrix (ndarray) for all class labels given in class_label_list.\n",
    "    The order of class_label_list is preserved.\n",
    "    The first returned dimension(rows) are the predicted labels, the second one(columns) are the true labels.\n",
    "    \"\"\"\n",
    "    confusion_matrix = []\n",
    "    for class_label_pred in class_label_list:\n",
    "        class_row = []\n",
    "        for class_label_true in class_label_list:\n",
    "            bool_pred = (y_pred == class_label_pred)\n",
    "            bool_true = (y_true == class_label_true)\n",
    "            occurrences = np.sum(np.logical_and(bool_pred, bool_true))\n",
    "            class_row.append(occurrences)\n",
    "        confusion_matrix.append(class_row)\n",
    "    return np.array(confusion_matrix)\n",
    "\n",
    "def display_confusion_matrix(y_pred, y_true, class_label_list):\n",
    "    \"\"\"Returns a labeled pandas DataFrame made from a confusion matrix (ndarray)\"\"\"\n",
    "    confusion_matrix = calculate_confusion_matrix(y_pred, y_true, class_label_list)\n",
    "    pred_labels = [\"pred: \" + str(x) for x in class_label_list]\n",
    "    true_labels = [\"true: \" + str(x) for x in class_label_list]\n",
    "    return pd.DataFrame(confusion_matrix, index=pred_labels, columns=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_y, pred_y, classes):\n",
    "        \"\"\"\n",
    "        Calculate accuracy for a classified set.\n",
    "        \"\"\"\n",
    "        class_sum = 0\n",
    "        for class_num in classes:\n",
    "            val_sum = 0\n",
    "            for true_val, pred_val in zip(true_y, pred_y): \n",
    "                if class_num == true_val:\n",
    "                    if true_val == pred_val:\n",
    "                        val_sum += 1\n",
    "            class_sum += val_sum\n",
    "        return class_sum / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_pred, y_test_changed, np.unique(y_test_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(y_test_changed, y_pred, np.unique(y_test_changed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Is AdaBoost better when using stronger weak learners? Why or why not? Compare your results to using depth-2 decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_depth2 = Ada_Boost(n_predictors = 5, tree_depth = 2)\n",
    "ada_depth2.train_ada_boost(X_train, y_train_changed)\n",
    "y_pred_depth2 = ada_depth2.predict(X_test)\n",
    "y_pred_depth2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_pred_depth2, y_test_changed, np.unique(y_test_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(y_test_changed, y_pred_depth2, np.unique(y_test_changed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2 (Bonus). Viola-Jones Face Detection\n",
    "Implement the Viola-Jones algorithm (without the cascade mechanism) and use it on a\n",
    "LFW-Face-subset\n",
    "to classify faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Visualize the top ten face classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 3 (Bonus). Cascade-Classification\n",
    "Implement a cascade algorithm to classify faces in a picture of your choice (there should be\n",
    "more than a face on your image, e.g. skimage.data.astronaut())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
