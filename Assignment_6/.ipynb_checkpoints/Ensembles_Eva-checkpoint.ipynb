{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('/Users/Eva/Downloads/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Implement a classification tree using python (incl. Numpy etc.) and use it on the SPAM-Dataset. Use a metric of your choice as a loss function.\n",
    "\n",
    "(Implementation inspired by:\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450, 57), (1151, 57))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450,), (1151,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible modifications/improvements:\n",
    "\n",
    "- Another possible stopping criterion: until all leaves are pure\n",
    "- Pruning to correct the depth that was added due to balance criterion: in case a node was split into a very unbalanced distribution where one child node contains no points and the other child contains all, then this split could be removed in postprocessing and the parent node can be made leaf\n",
    "- With my code it can happen that two (or more?) subsequent node levels test the same variable and value and assign the same labels to both sides...either bug fix or prune\n",
    "- Does the gini index need to be additionally weighted by group proportions?\n",
    "- Training on the complete data takes around 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree():\n",
    "\n",
    "\n",
    "    def __init__(self, node_min_size=2, tree_max_depth=None):\n",
    "        \"\"\"\n",
    "        If no maximum depth is defined, the nodes are split while they contain at least node_min_size many points.\n",
    "        If maximum depth is defined, nodes are split further until they contain less than node_min_size many points or\n",
    "        have reached the maximum number of allowed split levels.\n",
    "        \"\"\"\n",
    "        self.depth = 1\n",
    "        self.node_min_size = node_min_size\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        \n",
    "    ################ Tree building ################\n",
    "    \n",
    "    def split_data(self, X, j, v):\n",
    "        \"\"\"\n",
    "        Split data X at variable with index j and splitting point v.\n",
    "        \"\"\"\n",
    "        split_left = X[np.where(X[:,j]<=v)]\n",
    "        split_right = X[np.where(X[:,j]>v)]\n",
    "        return split_left, split_right\n",
    "\n",
    "    \n",
    "    def gini_index(self, groups):\n",
    "        \"\"\"\n",
    "        Calculate gini index for a data set splitted on a certain variable and splitting point.\n",
    "        groups: two subsets that result from splitting a data set\n",
    "        y: associated class labels\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        n = len(groups[0])+len(groups[1])\n",
    "        # get class proportions for each split group\n",
    "        for group in groups:\n",
    "            size = float(len(group))\n",
    "            y = group[:,-1]\n",
    "            if size == 0:\n",
    "                continue    \n",
    "            score = 0\n",
    "            # sum class proportions within one split groups as sum(p_mk*(1-p_mk))\n",
    "            for c in set(y):\n",
    "                proportions = np.count_nonzero(y==c)/size\n",
    "                score += proportions*(1-proportions)\n",
    "            # sum \"inversely squared\" class proportions per split group in weighted sum (weighted by proportion of group size)\n",
    "            gini += score * size/n \n",
    "        return gini\n",
    "\n",
    "\n",
    "    def split_node(self, data):\n",
    "        \"\"\"\n",
    "        Find optimal split for a given node \"node\" and class labels \"y\" that minimizes the loss function (gini index).\n",
    "        \"\"\"\n",
    "        split_var, split_point, split_gini, groups = 9999, 9999, 9999, None\n",
    "        # for every candidate split X*p* calculate gini index\n",
    "        for j in range(data.shape[1]-1):\n",
    "            candidate_p = np.unique(X[:,j])\n",
    "            for v in candidate_p:\n",
    "                split_groups = self.split_data(data, j, v)\n",
    "                s = self.gini_index(split_groups) \n",
    "                if s<split_gini: \n",
    "                    split_var, split_point, split_gini, groups = j, v, s, split_groups\n",
    "        return {'split_var': split_var, 'split_point': split_point, 'split_gini': split_gini, 'groups': groups}\n",
    "\n",
    "    \n",
    "    def make_leaf(self, node_labels):\n",
    "        \"\"\"\n",
    "        Convert node \"node\" to leaf by assigning it the majority vote of labels \"y\".\n",
    "        \"\"\"\n",
    "        leaf_labels = list(node_labels)\n",
    "        return max(set(leaf_labels), key=leaf_labels.count)\n",
    "\n",
    "    \n",
    "    def grow(self, node):\n",
    "        \"\"\"\n",
    "        Recusively split nodes until stopping criteria are fulfilled,\n",
    "        i.e. a node has too little data points in its region to be splitted further or the the has\n",
    "        reached its maximum allowd depth.\n",
    "        \"\"\"\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # Check for tree balance\n",
    "        if not left.any() or not right.any():\n",
    "            node['left'] = node['right'] = self.make_leaf(np.r_[left[:,-1], right[:,-1]])\n",
    "            return None\n",
    "        # Check for max depth\n",
    "        if self.tree_max_depth:\n",
    "            if self.depth >= self.tree_max_depth:\n",
    "                node['left'] = self.make_leaf(left[:,-1])\n",
    "                node['right'] = self.make_leaf(right[:,-1])\n",
    "                return None\n",
    "        # Check left node size\n",
    "        if len(left) <= self.node_min_size-1:\n",
    "            node['left'] = self.make_leaf(left[:,-1])\n",
    "        else:\n",
    "            node['left'] = self.split_node(left)\n",
    "            self.depth += 1\n",
    "            self.grow(node['left'])\n",
    "        # Check right node size\n",
    "        if len(right) <= self.node_min_size:\n",
    "            node['right'] = self.make_leaf(right[:,-1])\n",
    "        else:\n",
    "            node['right'] = self.split_node(right)\n",
    "            self.depth += 1\n",
    "            self.grow(node['right'])\n",
    "        return None\n",
    "\n",
    "    def grow_tree(self, X, y):\n",
    "        \"\"\"\n",
    "        Grow a decision tree for feature data X with class labels y that has at most tree_max_depth many levels\n",
    "        and at least node_mins_size many data points that support each leaf prediction.\n",
    "        \"\"\"\n",
    "        if X.shape[0] <= self.node_min_size:\n",
    "            raise Exception(\"Data set is too small to build a tree!\")\n",
    "        root = self.split_node(np.c_[X,y])\n",
    "        self.grow(root)\n",
    "        return root\n",
    "    \n",
    "    ################ Classification ################\n",
    "    \n",
    "    def get_label(self, test, subtree):\n",
    "        if type(subtree)==dict:\n",
    "            if test[subtree['split_var']] <= subtree['split_point']:\n",
    "                subtree = subtree['left']\n",
    "                return decide(test, subtree)\n",
    "            else:\n",
    "                subtree = subtree['right']\n",
    "                return decide(test, subtree)\n",
    "        else:\n",
    "            return subtree\n",
    "        \n",
    "        \n",
    "    def predict(self, tree, X):\n",
    "        y_pred = []\n",
    "        for item in X:\n",
    "            y_pred.append(self.get_label(item, tree))\n",
    "        return y_pred\n",
    "    \n",
    "    ################ Evaluation ################\n",
    "    \n",
    "    def calculate_accuracy(self, true_y, pred_y, classes):\n",
    "        \"\"\"\n",
    "        Calculate accuracy for a classified set.\n",
    "        \"\"\"\n",
    "        class_sum = 0\n",
    "        for class_num in classes:\n",
    "            val_sum = 0\n",
    "            for true_val, pred_val in zip(true_y, pred_y): \n",
    "                if class_num == true_val:\n",
    "                    if true_val == pred_val:\n",
    "                        val_sum += 1\n",
    "            class_sum += val_sum\n",
    "        return class_sum / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CT = ClassificationTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = CT.grow_tree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = CT.predict(tree, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 100*CT.calculate_accuracy(y_test, y_pred, set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained classification tree yields an accuracy of 91.57% on the test set\n"
     ]
    }
   ],
   "source": [
    "print(\"The trained classification tree yields an accuracy of {}% on the test set\".format(round(acc,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.a**\n",
    "\n",
    "Assume that classifying a genuine E-Mail as spam is ten times worse than classifying spam as genuine. How yould you change the design of your decision tree?\n",
    "\n",
    "\n",
    "(Eva): \n",
    "<br>Should we only answer or also implement here? My idea would be to use the misclassification error as loss function and to implement this penalty as a weighting scheme. <br> E.g. when calculating the misclassification error for a split, one node would have the maximum p_m,k for k=0 (real mail) and the other node would have maximum p_m,k for k=1 (spam). Then, the misclassification error for the p_m,0-node could be weighted with w_fp (weight for false positives) and the p_m,1.node could be weighted with w_fn (false negative weight) where w_fn = 10 * w_fp.\n",
    "Of course a similar weighting could be implemented into the gini scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.b**\n",
    "\n",
    "Use your tree to analyze feature importance. Plot the difference between the top 5 features (check spambase.names to check what features those belong to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
