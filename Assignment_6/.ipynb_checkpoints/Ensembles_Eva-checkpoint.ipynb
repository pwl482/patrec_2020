{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('/Users/Eva/Downloads/spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Implement a classification tree using python (incl. Numpy etc.) and use it on the SPAM-Dataset. Use a metric of your choice as a loss function.\n",
    "\n",
    "(Implementation inspired by:\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450, 57), (1151, 57))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450,), (1151,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible modifications/improvements:**\n",
    "\n",
    "1. Another possible stopping criterion: until all leaves are pure\n",
    "\n",
    "\n",
    "2. Pruning to correct the depth that was added due to balance criterion: in case a node was split into a very unbalanced distribution where one child node contains no points and the other child contains all, then this split could be removed in postprocessing and the parent node can be made leaf\n",
    "\n",
    "    => With my code it can happen that two (or more?) subsequent node levels test the same variable and value and assign the same labels to both sides...either bug fix or prune. I implemented a pruning protocol that summarizes nodes with leaves of the same class label into a leaf. Comparing the test set performace resulted in around 91.57% accuracy for both the unpruned (depth 411) and pruned tree (depth 299).\n",
    "    \n",
    "    \n",
    "3. Does the gini index need to be additionally weighted by group proportions?\n",
    "\n",
    "\n",
    "\n",
    "4. Training on the complete data takes around 3-4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree():\n",
    "\n",
    "\n",
    "    def __init__(self, node_min_size=2, tree_max_depth=None, post_pruning=True):\n",
    "        \"\"\"\n",
    "        If no maximum depth is defined, the nodes are split while they contain at least node_min_size many points.\n",
    "        If maximum depth is defined, nodes are split further until they contain less than node_min_size many points or\n",
    "        have reached the maximum number of allowed split levels.\n",
    "        \"\"\"\n",
    "        self.depth = 1\n",
    "        self.node_min_size = node_min_size\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        self.prune = 0\n",
    "        self.post_pruning = post_pruning\n",
    "        self.feature_importances = defaultdict(int)\n",
    "        \n",
    "    ################ Tree building ################\n",
    "    \n",
    "    def split_data(self, X, j, v):\n",
    "        \"\"\"\n",
    "        Split data X at variable with index j and splitting point v.\n",
    "        \"\"\"\n",
    "        split_left = X[np.where(X[:,j]<=v)]\n",
    "        split_right = X[np.where(X[:,j]>v)]\n",
    "        return split_left, split_right\n",
    "\n",
    "    \n",
    "    def gini_index(self, groups):\n",
    "        \"\"\"\n",
    "        Calculate gini index for a data set splitted on a certain variable and splitting point.\n",
    "        groups: two subsets that result from splitting a data set\n",
    "        y: associated class labels\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        if type(groups[0]) == np.ndarray:\n",
    "            n = sum(map(len,groups))\n",
    "            # get class proportions for each split group\n",
    "            for group in groups:\n",
    "                size = float(len(group))\n",
    "                y = group[:,-1]\n",
    "                if size == 0:\n",
    "                    continue    \n",
    "                score = 0\n",
    "                # sum class proportions within one split groups as sum(p_mk*(1-p_mk))\n",
    "                for c in set(y):\n",
    "                    proportions = np.count_nonzero(y==c)/size\n",
    "                    score += proportions*(1-proportions)\n",
    "                # sum \"inversely squared\" class proportions per split group in weighted sum (weighted by proportion of group size)\n",
    "                gini += score * size/n\n",
    "        else:\n",
    "            size = len(groups)\n",
    "            for c in set(groups):\n",
    "                proportions = groups.count(c)/size\n",
    "                gini += proportions*(1-proportions)\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def split_node(self, data):\n",
    "        \"\"\"\n",
    "        Find optimal split for a given node \"node\" and class labels \"y\" that minimizes the loss function (gini index).\n",
    "        \"\"\"\n",
    "        split_var, split_point, split_gini, groups = 9999, 9999, 9999, None\n",
    "        # for every candidate split X*p* calculate gini index and select split which minimizes the score\n",
    "        for j in range(data.shape[1]-1):\n",
    "            candidate_p = np.unique(X[:,j])\n",
    "            for v in candidate_p:\n",
    "                split_groups = self.split_data(data, j, v)\n",
    "                s = self.gini_index(split_groups) \n",
    "                if s < split_gini: \n",
    "                    split_var, split_point, split_gini, groups = j, v, s, split_groups\n",
    "        return {'split_var': split_var, 'split_point': split_point, 'split_gini': split_gini, 'groups': groups}\n",
    "\n",
    "\n",
    "    def make_leaf(self, node_labels):\n",
    "        \"\"\"\n",
    "        Convert node \"node\" to leaf by assigning it the majority vote of labels \"y\".\n",
    "        \"\"\"\n",
    "        leaf_labels = list(node_labels)\n",
    "        return {'label': max(set(leaf_labels), key=leaf_labels.count), 'class_sizes': [leaf_labels.count(c) for c in set(leaf_labels)]} \n",
    "\n",
    "    \n",
    "    def grow(self, node):\n",
    "        \"\"\"\n",
    "        Recusively split nodes until stopping criteria are fulfilled,\n",
    "        i.e. a node has too little data points in its region to be splitted further or the the has\n",
    "        reached its maximum allowd depth.\n",
    "        \"\"\"\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # Check for empty nodes (pruned in post-processing)\n",
    "        if not left.any() or not right.any():\n",
    "            node['left'] = node['right'] = self.make_leaf(np.r_[left[:,-1], right[:,-1]])\n",
    "            return None\n",
    "        # Check for max depth\n",
    "        if self.tree_max_depth:\n",
    "            if self.depth >= self.tree_max_depth:\n",
    "                node['left'] = self.make_leaf(left[:,-1])\n",
    "                node['right'] = self.make_leaf(right[:,-1])\n",
    "                return None\n",
    "        # Check left node size\n",
    "        if len(left) <= self.node_min_size-1:\n",
    "            node['left'] = self.make_leaf(left[:,-1])\n",
    "        else:\n",
    "            node['left'] = self.split_node(left)\n",
    "            self.depth += 1\n",
    "            self.grow(node['left'])\n",
    "        # Check right node size\n",
    "        if len(right) <= self.node_min_size:\n",
    "            node['right'] = self.make_leaf(right[:,-1])\n",
    "        else:\n",
    "            node['right'] = self.split_node(right)\n",
    "            self.depth += 1\n",
    "            self.grow(node['right'])\n",
    "        return None\n",
    "    \n",
    "    ################ Post-Pruning ################\n",
    "    \n",
    "    def check_twin_leaves(self, subtree):\n",
    "        \"\"\"\n",
    "        Prune tree while there still are twin leaves\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():  \n",
    "            if 'label' in subtree['left'].keys() and 'label' in subtree['right'].keys():\n",
    "                if subtree['left']['label'] == subtree['right']['label']:\n",
    "                    self.prune += 1\n",
    "            else:\n",
    "                self.check_twin_leaves(subtree['left'])\n",
    "                self.check_twin_leaves(subtree['right'])\n",
    "\n",
    "    def prune_tree(self, subtree):\n",
    "        \"\"\"\n",
    "        If a node results in leaves of the same label, remove leaves and make node to leaf\n",
    "        \"\"\"\n",
    "        # check if A is node or leaf\n",
    "        if not 'label' in subtree.keys():  \n",
    "            # at least one subtree B1, B2 has to be a node\n",
    "            if not 'label' in subtree['left'].keys() or not 'label' in subtree['right'].keys():\n",
    "                # prune left subtree B1?\n",
    "                if not 'label' in subtree['left'].keys():\n",
    "                    tmp = subtree['left']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['left'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['right'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "                # prune right subtree B2?           \n",
    "                if not 'label' in subtree['right'].keys():\n",
    "                    tmp = subtree['right']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['right'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['left'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "\n",
    "    ################ Build tree from data  ################\n",
    "    \n",
    "    def grow_tree(self, X, y):\n",
    "        \"\"\"\n",
    "        Grow a decision tree for feature data X with class labels y that has at most tree_max_depth many levels\n",
    "        and at least node_mins_size many data points that support each leaf prediction.\n",
    "        \"\"\"\n",
    "        print('Build tree from data')\n",
    "        if X.shape[0] <= self.node_min_size:\n",
    "            raise Exception(\"Data set is too small to build a tree!\")\n",
    "        root = self.split_node(np.c_[X,y])\n",
    "        self.grow(root)\n",
    "        if self.post_pruning:\n",
    "            print('Post-pruning')\n",
    "            self.check_twin_leaves(root)\n",
    "            while self.prune >= 1:\n",
    "                self.prune_tree(root)\n",
    "                self.check_twin_leaves(root)\n",
    "        print('Feature importances')\n",
    "        for i in range(X.shape[1]):\n",
    "            self.get_feature_importance(root,i)\n",
    "        f = float(sum(self.feature_importances.values()))\n",
    "        for k,v in self.feature_importances.items():\n",
    "            self.feature_importances[k] = v/f\n",
    "        return root   \n",
    "            \n",
    "    ################ Classification ################\n",
    "    \n",
    "    def get_label(self, test, subtree):\n",
    "        \"\"\"\n",
    "        Process data query through the decision tree and retrieve resulting leaf label.\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if test[subtree['split_var']] <= subtree['split_point']:\n",
    "                subtree = subtree['left']\n",
    "                return self.get_label(test, subtree)\n",
    "            else:\n",
    "                subtree = subtree['right']\n",
    "                return self.get_label(test, subtree)\n",
    "        else:\n",
    "            return subtree['label']\n",
    "        \n",
    "        \n",
    "    def predict(self, tree, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for data set X.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for item in X:\n",
    "            y_pred.append(self.get_label(item, tree))\n",
    "        return y_pred\n",
    "    \n",
    "    ################ Evaluation ################\n",
    "    \n",
    "    def calculate_accuracy(self, true_y, pred_y, classes):\n",
    "        \"\"\"\n",
    "        Calculate accuracy for a classified set.\n",
    "        \"\"\"\n",
    "        class_sum = 0\n",
    "        for class_num in classes:\n",
    "            val_sum = 0\n",
    "            for true_val, pred_val in zip(true_y, pred_y): \n",
    "                if class_num == true_val:\n",
    "                    if true_val == pred_val:\n",
    "                        val_sum += 1\n",
    "            class_sum += val_sum\n",
    "        return class_sum / len(true_y)\n",
    "    \n",
    "    \n",
    "    # TODO: weight scores by probability to reach respective node, i.e. node size/data size\n",
    "    def get_feature_importance(self, subtree, var):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if subtree['split_var'] == var:\n",
    "                if 'class_sizes' in subtree['left'].keys():\n",
    "                    ginil = gini_index(subtree['left']['class_sizes'])\n",
    "                else: \n",
    "                    ginil = subtree['left']['split_gini']\n",
    "                if 'class_sizes' in subtree['right'].keys():\n",
    "                    ginir = gini_index(subtree['right']['class_sizes'])\n",
    "                else:\n",
    "                    ginir = subtree['right']['split_gini']\n",
    "                self.feature_importances[var] += subtree['split_var']-(ginir+ginil)\n",
    "\n",
    "            self.get_feature_importance(subtree['left'], var)\n",
    "            self.get_feature_importance(subtree['right'], var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CT_pruned = ClassificationTree()\n",
    "#CT_unpruned = ClassificationTree(post_pruning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n"
     ]
    }
   ],
   "source": [
    "tree_pruned = CT_pruned.grow_tree(X_train[:100,:], y_train[:100])\n",
    "#tree_unpruned = CT_unpruned.grow_tree(X_train[:50,:], y_train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CT_pruned.depth#, CT_unpruned.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_pruned = CT_pruned.predict(tree_pruned, X_test[:50])\n",
    "#y_pred_unpruned = CT_unpruned.predict(tree_unpruned, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pruned = 100*CT_pruned.calculate_accuracy(y_test[:50], y_pred_pruned, set(y_test))\n",
    "#acc_unpruned = 100*CT_unpruned.calculate_accuracy(y_test, y_pred_unpruned, set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained pruned classification tree yields an accuracy of 78.0% on the test set\n"
     ]
    }
   ],
   "source": [
    "print(\"The trained pruned classification tree yields an accuracy of {}% on the test set\".format(round(acc_pruned,2)))\n",
    "#print(\"The trained unpruned classification tree yields an accuracy of {}% on the test set\".format(round(acc_unpruned,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.a**\n",
    "\n",
    "Assume that classifying a genuine E-Mail as spam is ten times worse than classifying spam as genuine. How yould you change the design of your decision tree?\n",
    "\n",
    "\n",
    "(Eva): \n",
    "<br>Should we only answer or also implement here? My idea would be to use the misclassification error as loss function and to implement this penalty as a weighting scheme. <br> E.g. when calculating the misclassification error for a split, one node would have the maximum p_m,k for k=0 (real mail) and the other node would have maximum p_m,k for k=1 (spam). Then, the misclassification error for the p_m,0-node could be weighted with w_fp (weight for false positives) and the p_m,1.node could be weighted with w_fn (false negative weight) where w_fn = 10 * w_fp.\n",
    "Of course a similar weighting could be implemented into the gini scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.b**\n",
    "\n",
    "Use your tree to analyze feature importance. Plot the difference between the top 5 features (check spambase.names to check what features those belong to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = CT_pruned.feature_importances\n",
    "sorted_feature_importances = list(zip(*sorted(feature_importances.items(), key= lambda x: x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sorted_feature_importances[0]\n",
    "importance = sorted_feature_importances[1]\n",
    "pos = np.arange(len(features))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(pos, importance)\n",
    "ax.set_yticks(pos)\n",
    "ax.set_yticklabels(features) #TODO: add real variable names\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Importance scores')\n",
    "ax.set_title('5 most important features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
