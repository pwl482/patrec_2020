{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo9GWxKgTN1h"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.347720Z",
     "start_time": "2018-11-29T11:28:47.572823Z"
    },
    "id": "V7XaSv5wTN1i"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax8ea49_bkdb"
   },
   "source": [
    "Load the spam dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T11:28:48.406520Z",
     "start_time": "2018-11-29T11:28:48.349530Z"
    },
    "id": "sT2Hk2k-TN1i"
   },
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Implement a classification tree using python (incl. Numpy etc.) and use it on the SPAM-Dataset. Use a metric of your choice as a loss function.\n",
    "\n",
    "(Implementation inspired by:\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 57)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450, 57), (1151, 57))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3450,), (1151,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible modifications/improvements:**\n",
    "\n",
    "1. Another possible stopping criterion: until all leaves are pure\n",
    "\n",
    "\n",
    "2. Pruning to correct the depth that was added due to balance criterion: in case a node was split into a very unbalanced distribution where one child node contains no points and the other child contains all, then this split could be removed in postprocessing and the parent node can be made leaf\n",
    "\n",
    "    => With my code it can happen that two (or more?) subsequent node levels test the same variable and value and assign the same labels to both sides...either bug fix or prune. I implemented a pruning protocol that summarizes nodes with leaves of the same class label into a leaf. Comparing the test set performace resulted in around 91.57% accuracy for both the unpruned (depth 411) and pruned tree (depth 299).\n",
    "    \n",
    "    \n",
    "3. Does the gini index need to be additionally weighted by group proportions?\n",
    "\n",
    "\n",
    "\n",
    "4. Training on the complete data takes around 3-4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree():\n",
    "\n",
    "\n",
    "    def __init__(self, node_min_size=2, tree_max_depth=None, post_pruning=True):\n",
    "        \"\"\"\n",
    "        If no maximum depth is defined, the nodes are split while they contain at least node_min_size many points.\n",
    "        If maximum depth is defined, nodes are split further until they contain less than node_min_size many points or\n",
    "        have reached the maximum number of allowed split levels.\n",
    "        \"\"\"\n",
    "        self.depth = 1\n",
    "        self.node_min_size = node_min_size\n",
    "        self.tree_max_depth = tree_max_depth\n",
    "        self.prune = 0\n",
    "        self.post_pruning = post_pruning\n",
    "        self.feature_importances = defaultdict(int)\n",
    "        \n",
    "    ################ Tree building ################\n",
    "    \n",
    "    def split_data(self, X, j, v):\n",
    "        \"\"\"\n",
    "        Split data X at variable with index j and splitting point v.\n",
    "        \"\"\"\n",
    "        split_left = X[np.where(X[:,j]<=v)]\n",
    "        split_right = X[np.where(X[:,j]>v)]\n",
    "        return split_left, split_right\n",
    "\n",
    "    \n",
    "    def gini_index(self, groups):\n",
    "        \"\"\"\n",
    "        Calculate gini index for a data set splitted on a certain variable and splitting point.\n",
    "        groups: two subsets that result from splitting a data set\n",
    "        y: associated class labels\n",
    "        \"\"\"\n",
    "        gini = 0\n",
    "        if type(groups[0]) == np.ndarray:\n",
    "            n = sum(map(len,groups))\n",
    "            # get class proportions for each split group\n",
    "            for group in groups:\n",
    "                size = float(len(group))\n",
    "                y = group[:,-1]\n",
    "                if size == 0:\n",
    "                    continue    \n",
    "                score = 0\n",
    "                # sum class proportions within one split groups as sum(p_mk*(1-p_mk))\n",
    "                for c in set(y):\n",
    "                    proportions = np.count_nonzero(y==c)/size\n",
    "                    score += proportions*(1-proportions)\n",
    "                # sum \"inversely squared\" class proportions per split group in weighted sum (weighted by proportion of group size)\n",
    "                gini += score * size/n\n",
    "        else:\n",
    "            size = len(groups)\n",
    "            for c in set(groups):\n",
    "                proportions = groups.count(c)/size\n",
    "                gini += proportions*(1-proportions)\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def split_node(self, data):\n",
    "        \"\"\"\n",
    "        Find optimal split for a given node \"node\" and class labels \"y\" that minimizes the loss function (gini index).\n",
    "        \"\"\"\n",
    "        split_var, split_point, split_gini, groups = 9999, 9999, 9999, None\n",
    "        # for every candidate split X*p* calculate gini index and select split which minimizes the score\n",
    "        for j in range(data.shape[1]-1):\n",
    "            candidate_p = np.unique(X[:,j])\n",
    "            for v in candidate_p:\n",
    "                split_groups = self.split_data(data, j, v)\n",
    "                s = self.gini_index(split_groups) \n",
    "                if s < split_gini: \n",
    "                    split_var, split_point, split_gini, groups = j, v, s, split_groups\n",
    "        return {'split_var': split_var, 'split_point': split_point, 'split_gini': split_gini, 'groups': groups}\n",
    "\n",
    "\n",
    "    def make_leaf(self, node_labels):\n",
    "        \"\"\"\n",
    "        Convert node \"node\" to leaf by assigning it the majority vote of labels \"y\".\n",
    "        \"\"\"\n",
    "        leaf_labels = list(node_labels)\n",
    "        return {'label': max(set(leaf_labels), key=leaf_labels.count), 'class_sizes': [leaf_labels.count(c) for c in set(leaf_labels)]} \n",
    "\n",
    "    \n",
    "    def grow(self, node):\n",
    "        \"\"\"\n",
    "        Recusively split nodes until stopping criteria are fulfilled,\n",
    "        i.e. a node has too little data points in its region to be splitted further or the the has\n",
    "        reached its maximum allowd depth.\n",
    "        \"\"\"\n",
    "        left, right = node['groups']\n",
    "        del(node['groups'])\n",
    "        # Check for empty nodes (pruned in post-processing)\n",
    "        if not left.any() or not right.any():\n",
    "            node['left'] = node['right'] = self.make_leaf(np.r_[left[:,-1], right[:,-1]])\n",
    "            return None\n",
    "        # Check for max depth\n",
    "        if self.tree_max_depth:\n",
    "            if self.depth >= self.tree_max_depth:\n",
    "                node['left'] = self.make_leaf(left[:,-1])\n",
    "                node['right'] = self.make_leaf(right[:,-1])\n",
    "                return None\n",
    "        # Check left node size\n",
    "        if len(left) <= self.node_min_size-1:\n",
    "            node['left'] = self.make_leaf(left[:,-1])\n",
    "        else:\n",
    "            node['left'] = self.split_node(left)\n",
    "            self.depth += 1\n",
    "            self.grow(node['left'])\n",
    "        # Check right node size\n",
    "        if len(right) <= self.node_min_size:\n",
    "            node['right'] = self.make_leaf(right[:,-1])\n",
    "        else:\n",
    "            node['right'] = self.split_node(right)\n",
    "            self.depth += 1\n",
    "            self.grow(node['right'])\n",
    "        return None\n",
    "    \n",
    "    ################ Post-Pruning ################\n",
    "    \n",
    "    def check_twin_leaves(self, subtree):\n",
    "        \"\"\"\n",
    "        Prune tree while there still are twin leaves\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():  \n",
    "            if 'label' in subtree['left'].keys() and 'label' in subtree['right'].keys():\n",
    "                if subtree['left']['label'] == subtree['right']['label']:\n",
    "                    self.prune += 1\n",
    "            else:\n",
    "                self.check_twin_leaves(subtree['left'])\n",
    "                self.check_twin_leaves(subtree['right'])\n",
    "\n",
    "    def prune_tree(self, subtree):\n",
    "        \"\"\"\n",
    "        If a node results in leaves of the same label, remove leaves and make node to leaf\n",
    "        \"\"\"\n",
    "        # check if A is node or leaf\n",
    "        if not 'label' in subtree.keys():  \n",
    "            # at least one subtree B1, B2 has to be a node\n",
    "            if not 'label' in subtree['left'].keys() or not 'label' in subtree['right'].keys():\n",
    "                # prune left subtree B1?\n",
    "                if not 'label' in subtree['left'].keys():\n",
    "                    tmp = subtree['left']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['left'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['right'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "                # prune right subtree B2?           \n",
    "                if not 'label' in subtree['right'].keys():\n",
    "                    tmp = subtree['right']\n",
    "                    # check if node has two leaves\n",
    "                    if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                        if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                            subtree['right'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                            self.prune -= 1\n",
    "                            # if B2 is a leaf, decrement depth\n",
    "                            if 'label' in subtree['left'].keys():\n",
    "                                self.depth -= 1\n",
    "                    else:\n",
    "                        self.prune_tree(tmp)\n",
    "\n",
    "    ################ Build tree from data  ################\n",
    "    \n",
    "    def grow_tree(self, X, y):\n",
    "        \"\"\"\n",
    "        Grow a decision tree for feature data X with class labels y that has at most tree_max_depth many levels\n",
    "        and at least node_mins_size many data points that support each leaf prediction.\n",
    "        \"\"\"\n",
    "        print('Build tree from data')\n",
    "        if X.shape[0] <= self.node_min_size:\n",
    "            raise Exception(\"Data set is too small to build a tree!\")\n",
    "        root = self.split_node(np.c_[X,y])\n",
    "        self.grow(root)\n",
    "        if self.post_pruning:\n",
    "            print('Post-pruning')\n",
    "            self.check_twin_leaves(root)\n",
    "            while self.prune >= 1:\n",
    "                self.prune_tree(root)\n",
    "                self.check_twin_leaves(root)\n",
    "        print('Feature importances')\n",
    "        for i in range(X.shape[1]):\n",
    "            self.get_feature_importance(root,i)\n",
    "        f = float(sum(self.feature_importances.values()))\n",
    "        for k,v in self.feature_importances.items():\n",
    "            self.feature_importances[k] = v/f\n",
    "        return root   \n",
    "            \n",
    "    ################ Classification ################\n",
    "    \n",
    "    def get_label(self, test, subtree):\n",
    "        \"\"\"\n",
    "        Process data query through the decision tree and retrieve resulting leaf label.\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if test[subtree['split_var']] <= subtree['split_point']:\n",
    "                subtree = subtree['left']\n",
    "                return self.get_label(test, subtree)\n",
    "            else:\n",
    "                subtree = subtree['right']\n",
    "                return self.get_label(test, subtree)\n",
    "        else:\n",
    "            return subtree['label']\n",
    "        \n",
    "        \n",
    "    def predict(self, tree, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for data set X.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for item in X:\n",
    "            y_pred.append(self.get_label(item, tree))\n",
    "        return y_pred\n",
    "    \n",
    "    ################ Evaluation ################\n",
    "    \n",
    "    def calculate_accuracy(self, true_y, pred_y, classes):\n",
    "        \"\"\"\n",
    "        Calculate accuracy for a classified set.\n",
    "        \"\"\"\n",
    "        class_sum = 0\n",
    "        for class_num in classes:\n",
    "            val_sum = 0\n",
    "            for true_val, pred_val in zip(true_y, pred_y): \n",
    "                if class_num == true_val:\n",
    "                    if true_val == pred_val:\n",
    "                        val_sum += 1\n",
    "            class_sum += val_sum\n",
    "        return class_sum / len(true_y)\n",
    "    \n",
    "    \n",
    "    # TODO: weight scores by probability to reach respective node, i.e. node size/data size\n",
    "    def get_feature_importance(self, subtree, var):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        if not 'label' in subtree.keys():\n",
    "            if subtree['split_var'] == var:\n",
    "                if 'class_sizes' in subtree['left'].keys():\n",
    "                    ginil = self.gini_index(subtree['left']['class_sizes'])\n",
    "                else: \n",
    "                    ginil = subtree['left']['split_gini']\n",
    "                if 'class_sizes' in subtree['right'].keys():\n",
    "                    ginir = self.gini_index(subtree['right']['class_sizes'])\n",
    "                else:\n",
    "                    ginir = subtree['right']['split_gini']\n",
    "                self.feature_importances[var] += subtree['split_var']-(ginir+ginil)\n",
    "\n",
    "            self.get_feature_importance(subtree['left'], var)\n",
    "            self.get_feature_importance(subtree['right'], var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CT_pruned = ClassificationTree()\n",
    "#CT_unpruned = ClassificationTree(post_pruning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n"
     ]
    }
   ],
   "source": [
    "tree_pruned = CT_pruned.grow_tree(X_train[:100,:], y_train[:100])\n",
    "#tree_unpruned = CT_unpruned.grow_tree(X_train[:50,:], y_train[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CT_pruned.depth#, CT_unpruned.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_pruned = CT_pruned.predict(tree_pruned, X_test[:50])\n",
    "#y_pred_unpruned = CT_unpruned.predict(tree_unpruned, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pruned = 100*CT_pruned.calculate_accuracy(y_test[:50], y_pred_pruned, set(y_test))\n",
    "#acc_unpruned = 100*CT_unpruned.calculate_accuracy(y_test, y_pred_unpruned, set(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The trained pruned classification tree yields an accuracy of {}% on the test set\".format(round(acc_pruned,2)))\n",
    "#print(\"The trained unpruned classification tree yields an accuracy of {}% on the test set\".format(round(acc_unpruned,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.a**\n",
    "\n",
    "Assume that classifying a genuine E-Mail as spam is ten times worse than classifying spam as genuine. How yould you change the design of your decision tree?\n",
    "\n",
    "\n",
    "(Eva): \n",
    "<br>Should we only answer or also implement here? My idea would be to use the misclassification error as loss function and to implement this penalty as a weighting scheme. <br> E.g. when calculating the misclassification error for a split, one node would have the maximum p_m,k for k=0 (real mail) and the other node would have maximum p_m,k for k=1 (spam). Then, the misclassification error for the p_m,0-node could be weighted with w_fp (weight for false positives) and the p_m,1.node could be weighted with w_fn (false negative weight) where w_fn = 10 * w_fp.\n",
    "Of course a similar weighting could be implemented into the gini scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.b**\n",
    "\n",
    "Use your tree to analyze feature importance. Plot the difference between the top 5 features (check spambase.names to check what features those belong to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = CT_pruned.feature_importances\n",
    "sorted_feature_importances = list(zip(*sorted(feature_importances.items(), key= lambda x: x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = sorted_feature_importances[0]\n",
    "importance = sorted_feature_importances[1]\n",
    "pos = np.arange(len(features))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(pos, importance)\n",
    "ax.set_yticks(pos)\n",
    "ax.set_yticklabels(features) #TODO: add real variable names\n",
    "ax.invert_yaxis() \n",
    "ax.set_xlabel('Importance scores')\n",
    "ax.set_title('5 most important features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2. Random Forests\n",
    "Implement a Random Forest and use it on the SPAM-Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, the Tree Building algorithm is exactly the same as before, except for the split node function. Instead of looping over all possible Xj of length N, we now only loop over a random subset of the Xj of length sqrt(N).\n",
    "Also the accuracy function was moved from the Classification Tree Object to outside of the objects, to avoid trying to access a class member fuction inside a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self):\n",
    "        self.trees = None\n",
    "        \n",
    "    class ClassificationTreeRF:\n",
    "        \n",
    "        def __init__(self, node_min_size=2, tree_max_depth=None, post_pruning=True):\n",
    "            \"\"\"\n",
    "            If no maximum depth is defined, the nodes are split while they contain at least node_min_size many points.\n",
    "            If maximum depth is defined, nodes are split further until they contain less than node_min_size many points or\n",
    "            have reached the maximum number of allowed split levels.\n",
    "            \"\"\"\n",
    "            self.depth = 1\n",
    "            self.node_min_size = node_min_size\n",
    "            self.tree_max_depth = tree_max_depth\n",
    "            self.prune = 0\n",
    "            self.post_pruning = post_pruning\n",
    "            self.feature_importances = defaultdict(int)\n",
    "\n",
    "        ################ Tree building ################\n",
    "\n",
    "        def split_data(self, X, j, v):\n",
    "            \"\"\"\n",
    "            Split data X at variable with index j and splitting point v.\n",
    "            \"\"\"\n",
    "            split_left = X[np.where(X[:,j]<=v)]\n",
    "            split_right = X[np.where(X[:,j]>v)]\n",
    "            return split_left, split_right\n",
    "\n",
    "\n",
    "        def gini_index(self, groups):\n",
    "            \"\"\"\n",
    "            Calculate gini index for a data set splitted on a certain variable and splitting point.\n",
    "            groups: two subsets that result from splitting a data set\n",
    "            y: associated class labels\n",
    "            \"\"\"\n",
    "            gini = 0\n",
    "            if type(groups[0]) == np.ndarray:\n",
    "                n = sum(map(len,groups))\n",
    "                # get class proportions for each split group\n",
    "                for group in groups:\n",
    "                    size = float(len(group))\n",
    "                    y = group[:,-1]\n",
    "                    if size == 0:\n",
    "                        continue    \n",
    "                    score = 0\n",
    "                    # sum class proportions within one split groups as sum(p_mk*(1-p_mk))\n",
    "                    for c in set(y):\n",
    "                        proportions = np.count_nonzero(y==c)/size\n",
    "                        score += proportions*(1-proportions)\n",
    "                    # sum \"inversely squared\" class proportions per split group in weighted sum (weighted by proportion of group size)\n",
    "                    gini += score * size/n\n",
    "            else:\n",
    "                size = len(groups)\n",
    "                for c in set(groups):\n",
    "                    proportions = groups.count(c)/size\n",
    "                    gini += proportions*(1-proportions)\n",
    "            return gini\n",
    "\n",
    "\n",
    "        def split_node(self, data):\n",
    "            \"\"\"\n",
    "            Find optimal split for a given node \"node\" and class labels \"y\" that minimizes the loss function (gini index).\n",
    "            Additionally for Random Forest, only a subset (sqrt(N)) of features is tested.\n",
    "            \"\"\"\n",
    "            split_var, split_point, split_gini, groups = 9999, 9999, 9999, None\n",
    "            # for selected candidates split X*p* calculate gini index and select split which minimizes the score\n",
    "            random_candidates = np.random.randint(data.shape[1]-1, size=int(np.sqrt(data.shape[1])))\n",
    "            for j in random_candidates:\n",
    "                candidate_p = np.unique(X[:,j])\n",
    "                for v in candidate_p:\n",
    "                    split_groups = self.split_data(data, j, v)\n",
    "                    s = self.gini_index(split_groups) \n",
    "                    if s < split_gini: \n",
    "                        split_var, split_point, split_gini, groups = j, v, s, split_groups\n",
    "            return {'split_var': split_var, 'split_point': split_point, 'split_gini': split_gini, 'groups': groups}\n",
    "\n",
    "\n",
    "        def make_leaf(self, node_labels):\n",
    "            \"\"\"\n",
    "            Convert node \"node\" to leaf by assigning it the majority vote of labels \"y\".\n",
    "            \"\"\"\n",
    "            leaf_labels = list(node_labels)\n",
    "            return {'label': max(set(leaf_labels), key=leaf_labels.count), 'class_sizes': [leaf_labels.count(c) for c in set(leaf_labels)]} \n",
    "\n",
    "\n",
    "        def grow(self, node):\n",
    "            \"\"\"\n",
    "            Recusively split nodes until stopping criteria are fulfilled,\n",
    "            i.e. a node has too little data points in its region to be splitted further or the the has\n",
    "            reached its maximum allowd depth.\n",
    "            \"\"\"\n",
    "            left, right = node['groups']\n",
    "            del(node['groups'])\n",
    "            # Check for empty nodes (pruned in post-processing)\n",
    "            if not left.any() or not right.any():\n",
    "                node['left'] = node['right'] = self.make_leaf(np.r_[left[:,-1], right[:,-1]])\n",
    "                return None\n",
    "            # Check for max depth\n",
    "            if self.tree_max_depth:\n",
    "                if self.depth >= self.tree_max_depth:\n",
    "                    node['left'] = self.make_leaf(left[:,-1])\n",
    "                    node['right'] = self.make_leaf(right[:,-1])\n",
    "                    return None\n",
    "            # Check left node size\n",
    "            if len(left) <= self.node_min_size-1:\n",
    "                node['left'] = self.make_leaf(left[:,-1])\n",
    "            else:\n",
    "                node['left'] = self.split_node(left)\n",
    "                self.depth += 1\n",
    "                self.grow(node['left'])\n",
    "            # Check right node size\n",
    "            if len(right) <= self.node_min_size:\n",
    "                node['right'] = self.make_leaf(right[:,-1])\n",
    "            else:\n",
    "                node['right'] = self.split_node(right)\n",
    "                self.depth += 1\n",
    "                self.grow(node['right'])\n",
    "            return None\n",
    "\n",
    "        ################ Post-Pruning ################\n",
    "\n",
    "        def check_twin_leaves(self, subtree):\n",
    "            \"\"\"\n",
    "            Prune tree while there still are twin leaves\n",
    "            \"\"\"\n",
    "            if not 'label' in subtree.keys():  \n",
    "                if 'label' in subtree['left'].keys() and 'label' in subtree['right'].keys():\n",
    "                    if subtree['left']['label'] == subtree['right']['label']:\n",
    "                        self.prune += 1\n",
    "                else:\n",
    "                    self.check_twin_leaves(subtree['left'])\n",
    "                    self.check_twin_leaves(subtree['right'])\n",
    "\n",
    "        def prune_tree(self, subtree):\n",
    "            \"\"\"\n",
    "            If a node results in leaves of the same label, remove leaves and make node to leaf\n",
    "            \"\"\"\n",
    "            # check if A is node or leaf\n",
    "            if not 'label' in subtree.keys():  \n",
    "                # at least one subtree B1, B2 has to be a node\n",
    "                if not 'label' in subtree['left'].keys() or not 'label' in subtree['right'].keys():\n",
    "                    # prune left subtree B1?\n",
    "                    if not 'label' in subtree['left'].keys():\n",
    "                        tmp = subtree['left']\n",
    "                        # check if node has two leaves\n",
    "                        if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                            if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                                subtree['left'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                                self.prune -= 1\n",
    "                                # if B2 is a leaf, decrement depth\n",
    "                                if 'label' in subtree['right'].keys():\n",
    "                                    self.depth -= 1\n",
    "                        else:\n",
    "                            self.prune_tree(tmp)\n",
    "                    # prune right subtree B2?           \n",
    "                    if not 'label' in subtree['right'].keys():\n",
    "                        tmp = subtree['right']\n",
    "                        # check if node has two leaves\n",
    "                        if 'label' in tmp['left'] and 'label' in tmp['right']:\n",
    "                            if tmp['left']['label'] == tmp['right']['label'] :\n",
    "                                subtree['right'] = {'label': tmp['left']['label'], 'class_sizes':list(map(sum,list(zip(tmp['left']['class_sizes'],tmp['right']['class_sizes']))))}\n",
    "                                self.prune -= 1\n",
    "                                # if B2 is a leaf, decrement depth\n",
    "                                if 'label' in subtree['left'].keys():\n",
    "                                    self.depth -= 1\n",
    "                        else:\n",
    "                            self.prune_tree(tmp)\n",
    "\n",
    "        ################ Build tree from data  ################\n",
    "\n",
    "        def grow_tree(self, X, y):\n",
    "            \"\"\"\n",
    "            Grow a decision tree for feature data X with class labels y that has at most tree_max_depth many levels\n",
    "            and at least node_mins_size many data points that support each leaf prediction.\n",
    "            \"\"\"\n",
    "            print('Build tree from data')\n",
    "            if X.shape[0] <= self.node_min_size:\n",
    "                raise Exception(\"Data set is too small to build a tree!\")\n",
    "            root = self.split_node(np.c_[X,y])\n",
    "            self.grow(root)\n",
    "            if self.post_pruning:\n",
    "                print('Post-pruning')\n",
    "                self.check_twin_leaves(root)\n",
    "                while self.prune >= 1:\n",
    "                    self.prune_tree(root)\n",
    "                    self.check_twin_leaves(root)\n",
    "            print('Feature importances')\n",
    "            for i in range(X.shape[1]):\n",
    "                self.get_feature_importance(root,i)\n",
    "            f = float(sum(self.feature_importances.values()))\n",
    "            for k,v in self.feature_importances.items():\n",
    "                self.feature_importances[k] = v/f\n",
    "            return root   \n",
    "\n",
    "        ################ Classification ################\n",
    "\n",
    "        def get_label(self, test, subtree):\n",
    "            \"\"\"\n",
    "            Process data query through the decision tree and retrieve resulting leaf label.\n",
    "            \"\"\"\n",
    "            if not 'label' in subtree.keys():\n",
    "                if test[subtree['split_var']] <= subtree['split_point']:\n",
    "                    subtree = subtree['left']\n",
    "                    return self.get_label(test, subtree)\n",
    "                else:\n",
    "                    subtree = subtree['right']\n",
    "                    return self.get_label(test, subtree)\n",
    "            else:\n",
    "                return subtree['label']\n",
    "\n",
    "\n",
    "        def predict(self, tree, X):\n",
    "            \"\"\"\n",
    "            Predict class labels for data set X.\n",
    "            \"\"\"\n",
    "            y_pred = []\n",
    "            for item in X:\n",
    "                y_pred.append(self.get_label(item, tree))\n",
    "            return y_pred\n",
    "\n",
    "        ################ Evaluation ################\n",
    "\n",
    "        # TODO: weight scores by probability to reach respective node, i.e. node size/data size\n",
    "        def get_feature_importance(self, subtree, var):\n",
    "            \"\"\"\n",
    "            \"\"\"\n",
    "            if not 'label' in subtree.keys():\n",
    "                if subtree['split_var'] == var:\n",
    "                    if 'class_sizes' in subtree['left'].keys():\n",
    "                        ginil = self.gini_index(subtree['left']['class_sizes'])\n",
    "                    else: \n",
    "                        ginil = subtree['left']['split_gini']\n",
    "                    if 'class_sizes' in subtree['right'].keys():\n",
    "                        ginir = self.gini_index(subtree['right']['class_sizes'])\n",
    "                    else:\n",
    "                        ginir = subtree['right']['split_gini']\n",
    "                    self.feature_importances[var] += subtree['split_var']-(ginir+ginil)\n",
    "\n",
    "                self.get_feature_importance(subtree['left'], var)\n",
    "                self.get_feature_importance(subtree['right'], var)\n",
    "    \n",
    "    ###################################################################\n",
    "    #################  Random Forest Functions  #######################\n",
    "    ###################################################################\n",
    "    \n",
    "    def fitRF(self, X_train, y_train, num_bags, bag_size):\n",
    "        \"\"\"\n",
    "        This Function creates a specified number of bags and trees and stores the trees in self.trees.\n",
    "        \"\"\"\n",
    "        X_bags = []\n",
    "        Y_bags = []\n",
    "        for bag_i in range(num_bags):\n",
    "            indices = np.random.randint(X_train.shape[0], size=bag_size)\n",
    "            X_bags.append(X_train[indices,:])\n",
    "            Y_bags.append(y_train[indices])\n",
    "        \n",
    "        self.trees = []\n",
    "        for i, (X_bags_i, Y_bags_i) in enumerate(zip(X_bags, Y_bags)):\n",
    "            print(\"--------------------------\")\n",
    "            print(\"Train Tree i =\", i)\n",
    "            self.trees.append(self.ClassificationTreeRF().grow_tree(X_bags_i, Y_bags_i))\n",
    "        print(\"--------------------------\")\n",
    "        \n",
    "    def predictRF(self, X_test):\n",
    "        \"\"\"\n",
    "        This Function predicts a label for a given test set for all trees, \n",
    "        then the majority vote of all trees is chosen as the label.\n",
    "        \"\"\"\n",
    "        if not self.trees is None:\n",
    "            y_pred_per_tree = np.array([self.ClassificationTreeRF().predict(tree, X_test) for tree in self.trees]).T\n",
    "            #print(y_pred_per_tree)\n",
    "            return np.array([np.argmax(np.bincount(xi.astype(int))) for xi in y_pred_per_tree]).astype(float)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def calculate_accuracy(true_y, pred_y, classes):\n",
    "            \"\"\"\n",
    "            Calculate accuracy for a classified set.\n",
    "            \"\"\"\n",
    "            class_sum = 0\n",
    "            for class_num in classes:\n",
    "                val_sum = 0\n",
    "                for true_val, pred_val in zip(true_y, pred_y): \n",
    "                    if class_num == true_val:\n",
    "                        if true_val == pred_val:\n",
    "                            val_sum += 1\n",
    "                class_sum += val_sum\n",
    "            return class_sum / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Train Tree i = 0\n",
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n",
      "--------------------------\n",
      "Train Tree i = 1\n",
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n",
      "--------------------------\n",
      "Train Tree i = 2\n",
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n",
      "--------------------------\n",
      "Train Tree i = 3\n",
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n",
      "--------------------------\n",
      "Train Tree i = 4\n",
      "Build tree from data\n",
      "Post-pruning\n",
      "Feature importances\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "RF = RandomForest()\n",
    "num_bags = 5\n",
    "bag_size = int(X_train.shape[0] * 0.75)\n",
    "RF.fitRF(X_train, y_train, num_bags, bag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 1., 0.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = RF.predictRF(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9539530842745438"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(y_test, y_pred, np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Print a confusion matrix (you can use package implementations here).\n",
    "Here we used a function we already wrote in week 4 to display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_pred, y_true, class_label_list):\n",
    "    \"\"\"\n",
    "    Returns a confusion matrix (ndarray) for all class labels given in class_label_list.\n",
    "    The order of class_label_list is preserved.\n",
    "    The first returned dimension(rows) are the predicted labels, the second one(columns) are the true labels.\n",
    "    \"\"\"\n",
    "    confusion_matrix = []\n",
    "    for class_label_pred in class_label_list:\n",
    "        class_row = []\n",
    "        for class_label_true in class_label_list:\n",
    "            bool_pred = (y_pred == class_label_pred)\n",
    "            bool_true = (y_true == class_label_true)\n",
    "            occurrences = np.sum(np.logical_and(bool_pred, bool_true))\n",
    "            class_row.append(occurrences)\n",
    "        confusion_matrix.append(class_row)\n",
    "    return np.array(confusion_matrix)\n",
    "\n",
    "def display_confusion_matrix(y_pred, y_true, class_label_list):\n",
    "    \"\"\"Returns a labeled pandas DataFrame made from a confusion matrix (ndarray)\"\"\"\n",
    "    confusion_matrix = calculate_confusion_matrix(y_pred, y_true, class_label_list)\n",
    "    pred_labels = [\"pred: \" + str(x) for x in class_label_list]\n",
    "    true_labels = [\"true: \" + str(x) for x in class_label_list]\n",
    "    return pd.DataFrame(confusion_matrix, index=pred_labels, columns=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true: 0.0</th>\n",
       "      <th>true: 1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred: 0.0</th>\n",
       "      <td>669</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred: 1.0</th>\n",
       "      <td>28</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           true: 0.0  true: 1.0\n",
       "pred: 0.0        669         25\n",
       "pred: 1.0         28        429"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_confusion_matrix(y_pred, y_test, np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) What is a good number of trees in the forest?\n",
    "We try out different numbers of bags/trees, initially from 2-20 (but that takes quite a while, so reducing it to a smaller interval also works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "num_bags_list = np.arange(2,21) #<- change it for a shorter runtime\n",
    "bag_size2 = int(X_train.shape[0] * 0.75)\n",
    "accuracies = []\n",
    "for num_bags2 in num_bags_list:\n",
    "    RF = RandomForest()\n",
    "    RF.fitRF(X_train, y_train, num_bags2, bag_size2)\n",
    "    y_pred2 = RF.predictRF(X_test)\n",
    "    accuracies.append(calculate_accuracy(y_test, y_pred2, np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_bags_list, accuracies)\n",
    "plt.xlabel(\"Number of Bags/Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracies of Random Forest for different number of Bags/Trees\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "Seems like 16 is the best number of bags/trees"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensembles.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
