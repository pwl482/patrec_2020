{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "RNN_Assignment_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "75VOiRumbtCW"
      },
      "source": [
        "# Text Generation with RNNs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "q-eZcyy_btCX"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Kwc0o4bGbtCa"
      },
      "source": [
        "### 1 Dataset\n",
        "Define the path of the file, you want to read and train the model on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "_-pWsSdWbtCb"
      },
      "source": [
        "'''TODO: set the path of the file'''\n",
        "path_to_file = 'Shakespeare_karpathy.txt'\n",
        "text = open(path_to_file, encoding='utf-8').read()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ku1gyuOibtCd"
      },
      "source": [
        "\n",
        "#### Inspect the dataset\n",
        "Take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "hOoE3ZLwbtCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc1232d-6bed-48c5-ad90-f399cf874877"
      },
      "source": [
        "print(text[:250])\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That, poor contempt, or claim'd thou slept so faithful,\n",
            "I may contrive our father; and, in their defeated queen,\n",
            "Her flesh broke me and puttance of expedition house,\n",
            "And in that same that ever I lament this stomach,\n",
            "And he, nor Butly and my fury, kno\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% \n",
          "is_executing": false
        },
        "id": "ixxZ4yBqbtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de9d2a1-023c-469f-ce41-68794a818c48"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "luvcRu86btCj"
      },
      "source": [
        "### 2 Process the dataset for the learning task\n",
        "The task that we want our model to achieve is: given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction.\n",
        "\n",
        "#### Vectorize the text\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7koc3Q2dbtCk"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create a mapping from indices to characters\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s04cWuBCbtCm"
      },
      "source": [
        "\n",
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to len(unique). Let's take a peek at this numerical representation of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7Rbt2hfpbtCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94daff71-f51b-4cd0-e7cb-f891e18d8b38"
      },
      "source": [
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  \"'\" :   3,\n",
            "  ',' :   4,\n",
            "  '-' :   5,\n",
            "  '.' :   6,\n",
            "  ':' :   7,\n",
            "  ';' :   8,\n",
            "  '?' :   9,\n",
            "  'A' :  10,\n",
            "  'B' :  11,\n",
            "  'C' :  12,\n",
            "  'D' :  13,\n",
            "  'E' :  14,\n",
            "  'F' :  15,\n",
            "  'G' :  16,\n",
            "  'H' :  17,\n",
            "  'I' :  18,\n",
            "  'J' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Dsh270g9btCp"
      },
      "source": [
        "\n",
        "\n",
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "egRTUKhlbtCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf6b4056-5585-44a0-a9e6-272ecf7a1d1c"
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'That, poor co' ---- characters mapped to int ---- > [29 43 36 55  4  1 51 50 50 53  1 38 50]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fti3o5GHbtCs"
      },
      "source": [
        "#### Defining a method to encode one hot labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "oef5BSBpbtCt"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    print('onehot arrays')\n",
        "    # Finally reshape it to get back to the original array\n",
        "    print(one_hot.shape)\n",
        "    print(*arr.shape)\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    print(one_hot.shape)\n",
        "    return one_hot"
      ],
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RxoJlIhmbtCw"
      },
      "source": [
        "\n",
        "#### Defining a method to make mini-batches for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "1JNdq5YgbtCw"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "\n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n + seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "\n"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oHoy0u0abtCz"
      },
      "source": [
        "\n",
        "## 3 The Recurrent Neural Network (RNN) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tkdc8Z6VbtC0"
      },
      "source": [
        "\n",
        "###### Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "5wRmMM9kbtC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5af0aef-f8af-4d53-806a-e64d50c6a87b"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "print ('Training on GPU' if train_on_gpu else 'Training on CPU')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "C4b7JO6FbtC3"
      },
      "source": [
        "\n",
        "### Declaring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "k72_pm6obtC3"
      },
      "source": [
        "class VanillaCharRNN(nn.Module):\n",
        "    def __init__(self, vocab, n_hidden=256, n_layers=2,\n",
        "                 drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        '''TODO: define the layers you need for the model'''\n",
        "        self.rnn = nn.RNN(len(self.vocab), n_hidden, n_layers, batch_first=True, dropout=self.drop_prob)   #input and output tensors are provided as (batch, seq, feature)\n",
        "        self.fc = nn.Linear(n_hidden, len(vocab))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        '''TODO: Forward pass through the network\n",
        "        x is the input and `hidden` is the hidden/cell state .'''\n",
        "        # Initialize hidden state for first input using method defined below\n",
        "        #batch_size = x.size(0)\n",
        "        #hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        # x of shape (seq_len, batch, input_size)\n",
        "        # hidden of shape (num_layers * num_directions, batch, hidden_size)\n",
        "        out, hidden_t = self.rnn(x, hidden)\n",
        "        print('RNN out', out.shape)\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        print('linear out', out.shape)\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden_t\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFZqqjw013I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "0e00b904-f733-4495-97ab-f032f23943f7"
      },
      "source": [
        "class LSTMCharRNN(nn.Module):\n",
        "    def __init__(self, vocab, n_hidden=256, n_layers=2,\n",
        "                 drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        '''TODO: define the layers you need for the model'''\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        '''TODO: Forward pass through the network\n",
        "        x is the input and `hidden` is the hidden/cell state .'''\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden_t\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        \n",
        "        hidden = # TODO\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-128-fb82db04f6b0>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    hidden = # TODO\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hD6AFo9zbtC8"
      },
      "source": [
        "\n",
        "#### Declaring the train method\n",
        "\n",
        "\n",
        "train(vanilla_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "S2HKSmmAbtC9"
      },
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        model: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    '''\n",
        "    model.train()\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_vocab = len(model.vocab)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = model.init_hidden(batch_size)\n",
        "        print('init h', h.shape)\n",
        "        print('init h[0]', h[0].shape)\n",
        "        \n",
        "        '''TODO: use the get_batches function to generate sequences of the desired size'''\n",
        "        dataset = get_batches(data, batch_size, seq_length)\n",
        "\n",
        "        for x, y in dataset:\n",
        "            counter += 1\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_vocab)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if (train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "           \n",
        "            h = tuple([each.data for each in h])\n",
        "            print('inputs', inputs.shape, torch.cat(h).reshape([model.n_layers,batch_size,-1]).cuda().shape)\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "            '''TODO: feed the current input into the model and generate output'''\n",
        "            output, h = model(inputs, torch.cat(h).reshape([model.n_layers,batch_size,-1]).cuda()) \n",
        "            print('output', output.reshape([batch_size,seq_length]).shape, targets.shape)\n",
        "            '''TODO: compute the loss!'''\n",
        "            loss = criterion(output.reshape([batch_size,seq_length]), targets)\n",
        "            \n",
        "            # perform backprop\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_vocab)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if (train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    print('Feed2')\n",
        "                    '''TODO: feed the current input into the model and generate output'''\n",
        "                    output, val_h = model(inputs, val_h)\n",
        "                    \n",
        "                    '''TODO: compute the validation loss!'''\n",
        "                    val_loss = criterion(output, targets)\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                \n",
        "                '''TODO: sample from the model to generate texts'''\n",
        "                input_eval = 'Dear'\n",
        "                print(sample(model, 1000, prime=input_eval, top_k=10))\n",
        "                \n",
        "                model.train()  # reset to train mode after iterationg through validation data"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wtlmj6sAbtC_"
      },
      "source": [
        "\n",
        "##### Defining a method to generate the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "eTDAUosNbtC_"
      },
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "    ''' Given a character, predict the next character.\n",
        "        Returns the predicted character and the hidden state.\n",
        "    '''\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[char2idx[char]]])\n",
        "    x = one_hot_encode(x, len(model.vocab))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    print('Feed3')\n",
        "    '''TODO: feed the current input into the model and generate output'''\n",
        "    output, h = model(inputs, ----)\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    if (train_on_gpu):\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(model.vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return idx2char[char], h\n"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4a_I3GHqbtDB"
      },
      "source": [
        "\n",
        "#### Declaring a method to generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mS9UcwBXbtDC"
      },
      "source": [
        "def sample(model, size, prime='The', top_k=None):\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    model.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    for ii in range(size):\n",
        "      '''TODO: pass in the previous character and get a new one'''\n",
        "      char, h = predict(model, chars, h, top_k=top_k)\n",
        "      chars.append(char)\n",
        "\n",
        "    model.train()\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "viwhJM9RbtDE"
      },
      "source": [
        "\n",
        "#### Generate new Text using the RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7bc2y3DbtDE"
      },
      "source": [
        "###### Define and print the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mHbAsHB2btDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1482d512-c04d-4725-bdd9-90b82718dcef"
      },
      "source": [
        "''''TODO: Try changing the number of units in the network to see how it affects performance'''\n",
        "n_hidden = 256\n",
        "n_layers = 2\n",
        "\n",
        "vanilla_model = VanillaCharRNN(vocab, n_hidden, n_layers)\n",
        "print(vanilla_model)\n",
        "#lstm_model = LSTMCharRNN(vocab, n_hidden, n_layers)\n",
        "#print(lstm_model)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VanillaCharRNN(\n",
            "  (rnn): RNN(62, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=256, out_features=62, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e-sU3cQIbtDI"
      },
      "source": [
        "\n",
        "###### Declaring the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ZJnyYkQlbtDJ"
      },
      "source": [
        "''''TODO: Try changing the hyperparameters in the network to see how it affects performance'''\n",
        "batch_size = 10\n",
        "seq_length = 50\n",
        "n_epochs = 2  # start smaller if you are just testing initial behavior"
      ],
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "46LJ32XHbtDM"
      },
      "source": [
        "\n",
        "##### Train the model and have fun with the generated texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "rL0QLMXlbtDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a17831fa-7dfa-40fc-d9f6-62ce411348e1"
      },
      "source": [
        "''''TODO: compare the results from the vanilla and LSTM model'''\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'TODO: compare the results from the vanilla and LSTM model\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMQ9nhVfhvLk"
      },
      "source": [
        "**Note** \n",
        "\n",
        "According to the pytorch doc, the input tensor should have shape (seq_len, batch, input_size) and the hidden state should have shape (num_layers, batch, hidden_size). \n",
        "\n",
        "In this implementation, the encoding and batching produces dimensions (seq_len, batch, input_size) and (num_layers, seq_len, hidden_size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzc7Dz65FOPs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "outputId": "3b1c00a2-0da5-480f-a983-6a993cca9a3f"
      },
      "source": [
        "train(vanilla_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ],
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init h torch.Size([2, 10, 256])\n",
            "init h[0] torch.Size([10, 256])\n",
            "onehot arrays\n",
            "(500, 62)\n",
            "10 50\n",
            "(10, 50, 62)\n",
            "inputs torch.Size([10, 50, 62]) torch.Size([2, 10, 256])\n",
            "RNN out torch.Size([10, 50, 256])\n",
            "linear out torch.Size([500, 62])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-278-64a1450153a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvanilla_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_as_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-276-7304d9ec9530>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m'''TODO: feed the current input into the model and generate output'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m'''TODO: compute the loss!'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 50]' is invalid for input of size 31000"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdBcUbv6FObG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "e6e4f4ca-8020-414d-a195-83eb896e29f6"
      },
      "source": [
        "train(lstm_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-140-8d6efee58a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_as_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnI-lDmFFcpd"
      },
      "source": [
        ""
      ]
    }
  ]
}