{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "RNN_Assignment_13.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "75VOiRumbtCW"
      },
      "source": [
        "# Text Generation with RNNs\n",
        "https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
        "\n",
        "https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "q-eZcyy_btCX"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Kwc0o4bGbtCa"
      },
      "source": [
        "### 1 Dataset\n",
        "Define the path of the file, you want to read and train the model on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "_-pWsSdWbtCb"
      },
      "source": [
        "'''TODO: set the path of the file'''\n",
        "path_to_file = 'Shakespeare_karpathy.txt'\n",
        "text = open(path_to_file, encoding='utf-8').read()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ku1gyuOibtCd"
      },
      "source": [
        "\n",
        "#### Inspect the dataset\n",
        "Take a look at the first 250 characters in text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "hOoE3ZLwbtCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d30070f9-75aa-418e-eec7-1c88ef4c5a88"
      },
      "source": [
        "print(text[:250])\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That, poor contempt, or claim'd thou slept so faithful,\n",
            "I may contrive our father; and, in their defeated queen,\n",
            "Her flesh broke me and puttance of expedition house,\n",
            "And in that same that ever I lament this stomach,\n",
            "And he, nor Butly and my fury, kno\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%% \n",
          "is_executing": false
        },
        "id": "ixxZ4yBqbtCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35130043-41cc-49f9-aceb-a82f07b9fdeb"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "luvcRu86btCj"
      },
      "source": [
        "### 2 Process the dataset for the learning task\n",
        "The task that we want our model to achieve is: given a character, or a sequence of characters, what is the most probable next character?\n",
        "\n",
        "To achieve this, we will input a sequence of characters to the model, and train the model to predict the output, that is, the following character at each time step. RNNs maintain an internal state that depends on previously seen elements, so information about all characters seen up until a given moment will be taken into account in generating the prediction.\n",
        "\n",
        "#### Vectorize the text\n",
        "Before we begin training our RNN model, we'll need to create a numerical representation of our text-based dataset. To do this, we'll generate two lookup tables: one that maps characters to numbers, and a second that maps numbers back to characters. Recall that we just identified the unique characters present in the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7koc3Q2dbtCk"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create a mapping from indices to characters\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s04cWuBCbtCm"
      },
      "source": [
        "\n",
        "This gives us an integer representation for each character. Observe that the unique characters (i.e., our vocabulary) in the text are mapped as indices from 0 to len(unique). Let's take a peek at this numerical representation of our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7Rbt2hfpbtCn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2898f6-0e0f-47c9-f9a5-a7c89a09376e"
      },
      "source": [
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  \"'\" :   3,\n",
            "  ',' :   4,\n",
            "  '-' :   5,\n",
            "  '.' :   6,\n",
            "  ':' :   7,\n",
            "  ';' :   8,\n",
            "  '?' :   9,\n",
            "  'A' :  10,\n",
            "  'B' :  11,\n",
            "  'C' :  12,\n",
            "  'D' :  13,\n",
            "  'E' :  14,\n",
            "  'F' :  15,\n",
            "  'G' :  16,\n",
            "  'H' :  17,\n",
            "  'I' :  18,\n",
            "  'J' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Dsh270g9btCp"
      },
      "source": [
        "\n",
        "\n",
        "We can also look at how the first part of the text is mapped to an integer representation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "egRTUKhlbtCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1207e303-3557-4408-815e-ee26fdcbc04f"
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'That, poor co' ---- characters mapped to int ---- > [29 43 36 55  4  1 51 50 50 53  1 38 50]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fti3o5GHbtCs"
      },
      "source": [
        "#### Defining a method to encode one hot labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "oef5BSBpbtCt"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "\n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    return one_hot"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "RxoJlIhmbtCw"
      },
      "source": [
        "\n",
        "#### Defining a method to make mini-batches for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "1JNdq5YgbtCw"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "\n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n + seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oHoy0u0abtCz"
      },
      "source": [
        "\n",
        "## 3 The Recurrent Neural Network (RNN) model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Tkdc8Z6VbtC0"
      },
      "source": [
        "\n",
        "###### Check if GPU is available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "5wRmMM9kbtC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6a7802d-ab6f-47b8-c1d3-0ebaa6d2f3a2"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "print ('Training on GPU' if train_on_gpu else 'Training on CPU')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "C4b7JO6FbtC3"
      },
      "source": [
        "\n",
        "### Declaring the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "k72_pm6obtC3"
      },
      "source": [
        "class VanillaCharRNN(nn.Module):\n",
        "    def __init__(self, vocab, n_hidden=256, n_layers=2,\n",
        "                 drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        '''TODO: define the layers you need for the model'''\n",
        "        self.rnn = nn.RNN(len(self.vocab), self.n_hidden, self.n_layers, batch_first=True, dropout=self.drop_prob)   #batch_first=True -> input and output tensors are provided as (batch, seq, feature)\n",
        "        self.fc = nn.Linear(self.n_hidden, len(self.vocab))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        '''TODO: Forward pass through the network\n",
        "        x is the input and `hidden` is the hidden/cell state .'''\n",
        "\n",
        "        # Passing in the input and hidden state into the model and obtaining outputs\n",
        "        # x of shape (seq_len, batch, input_size)\n",
        "        # hidden of shape (num_layers * num_directions, batch, hidden_size)\n",
        "        out, hidden_t = self.rnn(x, hidden)\n",
        "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden_t\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden)\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKFZqqjw013I"
      },
      "source": [
        "class LSTMCharRNN(nn.Module):\n",
        "    def __init__(self, vocab, n_hidden=256, n_layers=2,\n",
        "                 drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        '''TODO: define the layers you need for the model'''\n",
        "        self.lstm = nn.LSTM(len(self.vocab), self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "        # add Dropout layer?\n",
        "        self.fc = nn.Linear(self.n_hidden, len(self.vocab))\n",
        "        # add sigmoid layer?\n",
        "\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        '''TODO: Forward pass through the network\n",
        "        x is the input and `hidden` is the hidden/cell state .'''\n",
        "        #print('lstm in', x.shape, hidden.shape)\n",
        "        cell = self.init_hidden(hidden.shape[1])\n",
        "        out, (hidden_t, cell_t)= self.lstm(x, (hidden, cell))\n",
        "        #print('lstm out', out.shape, hidden_t.shape)\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        out = self.fc(out)\n",
        "        #print('linear out', out.shape)\n",
        "       \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden_t\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        hidden = torch.zeros(self.n_layers, batch_size, self.n_hidden).cuda()\n",
        "        # cell state should be included as for now the model only backpropagates the loss over the hidden states and not the cell states\n",
        "        return hidden\n"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hD6AFo9zbtC8"
      },
      "source": [
        "\n",
        "#### Declaring the train method\n",
        "\n",
        "\n",
        "train(vanilla_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "S2HKSmmAbtC9"
      },
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "\n",
        "        model: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "\n",
        "    '''\n",
        "    model.train()\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data) * (1 - val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "\n",
        "    counter = 0\n",
        "    n_vocab = len(model.vocab)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = model.init_hidden(batch_size)\n",
        "        \n",
        "        '''TODO: use the get_batches function to generate sequences of the desired size'''\n",
        "        dataset = get_batches(data, batch_size, seq_length)\n",
        "\n",
        "        for x, y in dataset:\n",
        "            counter += 1\n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_vocab)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "            if (train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "            # zero accumulated gradients\n",
        "            model.zero_grad()\n",
        "            #print('inputs', inputs.shape, torch.cat(h).reshape([model.n_layers,batch_size,-1]).cuda().shape)\n",
        "            '''TODO: feed the current input into the model and generate output'''\n",
        "            output, h = model(inputs, torch.cat(h).reshape([model.n_layers,batch_size,-1]).cuda()) \n",
        "            '''TODO: compute the loss!'''\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "            \n",
        "            # perform backprop\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            opt.step()\n",
        "\n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = model.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                model.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_vocab)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                    inputs, targets = x, y\n",
        "                    if (train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                    #print('Feed2')\n",
        "                    '''TODO: feed the current input into the model and generate output'''\n",
        "                    output, val_h = model(inputs, torch.cat(val_h).reshape([model.n_layers,batch_size,-1]).cuda())\n",
        "\n",
        "                    '''TODO: compute the validation loss!'''\n",
        "                    val_loss = criterion(output, targets.view(-1))\n",
        "\n",
        "                    val_losses.append(val_loss.item())\n",
        "\n",
        "                print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                \n",
        "                '''TODO: sample from the model to generate texts'''\n",
        "                input_eval = 'Dear'\n",
        "                print(sample(model, 1000, prime=input_eval, top_k=10))\n",
        "                \n",
        "                model.train()  # reset to train mode after iterationg through validation data"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "wtlmj6sAbtC_"
      },
      "source": [
        "\n",
        "##### Defining a method to generate the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "eTDAUosNbtC_"
      },
      "source": [
        "def predict(model, char, h=None, top_k=None):\n",
        "    ''' Given a character, predict the next character.\n",
        "        Returns the predicted character and the hidden state.\n",
        "    '''\n",
        "\n",
        "    # tensor inputs\n",
        "    x = np.array([[char2idx[char]]])\n",
        "    x = one_hot_encode(x, len(model.vocab))\n",
        "    inputs = torch.from_numpy(x)\n",
        "\n",
        "    if (train_on_gpu):\n",
        "        inputs = inputs.cuda()\n",
        "\n",
        "    # detach hidden state from history\n",
        "    h = tuple([each.data for each in h])\n",
        "    #print('predict')\n",
        "    #print('h',len(h))\n",
        "    #print('h[0]', h[0].shape)\n",
        "    #print('Feed3')\n",
        "    '''TODO: feed the current input into the model and generate output'''\n",
        "    #print(torch.cat(h).reshape([model.n_layers,1,-1]).cuda().shape)\n",
        "    output, h = model(inputs, torch.cat(h).reshape([model.n_layers,1,-1]).cuda())\n",
        "\n",
        "    # get the character probabilities\n",
        "    p = F.softmax(output, dim=1).data\n",
        "    if (train_on_gpu):\n",
        "        p = p.cpu()  # move to cpu\n",
        "\n",
        "    # get top characters\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(model.vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    # select the likely next character with some element of randomness\n",
        "    p = p.numpy().squeeze()\n",
        "    char = np.random.choice(top_ch, p=p / p.sum())\n",
        "\n",
        "    # return the encoded value of the predicted char and the hidden state\n",
        "    return idx2char[char], h\n"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4a_I3GHqbtDB"
      },
      "source": [
        "\n",
        "#### Declaring a method to generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mS9UcwBXbtDC"
      },
      "source": [
        "def sample(model, size, prime='The', top_k=None):\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "\n",
        "    model.eval()  # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden(1)\n",
        "    #print('sample')\n",
        "    #print('h',len(h))\n",
        "    #print('h[0]', h[0].shape)\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    for ii in range(size):\n",
        "      '''TODO: pass in the previous character and get a new one'''\n",
        "      char, h = predict(model, char, h, top_k=top_k)\n",
        "      chars.append(char)\n",
        "\n",
        "    model.train()\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "viwhJM9RbtDE"
      },
      "source": [
        "\n",
        "#### Generate new Text using the RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7bc2y3DbtDE"
      },
      "source": [
        "###### Define and print the net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "mHbAsHB2btDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5800faa9-65d4-4ae0-999b-deeada7e7d4b"
      },
      "source": [
        "''''TODO: Try changing the number of units in the network to see how it affects performance'''\n",
        "n_hidden = 256\n",
        "n_layers = 2\n",
        "\n",
        "vanilla_model = VanillaCharRNN(vocab, n_hidden, n_layers)\n",
        "print(vanilla_model)\n",
        "lstm_model = LSTMCharRNN(vocab, n_hidden, n_layers)\n",
        "print(lstm_model)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VanillaCharRNN(\n",
            "  (rnn): RNN(62, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=256, out_features=62, bias=True)\n",
            ")\n",
            "LSTMCharRNN(\n",
            "  (lstm): LSTM(62, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=256, out_features=62, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "e-sU3cQIbtDI"
      },
      "source": [
        "\n",
        "###### Declaring the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "ZJnyYkQlbtDJ"
      },
      "source": [
        "''''TODO: Try changing the hyperparameters in the network to see how it affects performance'''\n",
        "batch_size = 10\n",
        "seq_length = 50\n",
        "n_epochs = 2  # start smaller if you are just testing initial behavior"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "46LJ32XHbtDM"
      },
      "source": [
        "\n",
        "##### Train the model and have fun with the generated texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "rL0QLMXlbtDN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d1cfcf2-4619-4444-d189-6983a32d9d43"
      },
      "source": [
        "\n",
        "''''TODO: compare the results from the vanilla and LSTM model'''\n"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"'TODO: compare the results from the vanilla and LSTM model\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzc7Dz65FOPs"
      },
      "source": [
        "train(vanilla_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdBcUbv6FObG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "954cf9d0-f536-45c5-ba90-b0e0a153d61e"
      },
      "source": [
        "train(lstm_model, text_as_int, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/2... Step: 50... Loss: 3.3567... Val Loss: 3.2897\n",
            "Dearreai  \n",
            "t ersr\n",
            "a roreieai noeiresnnaeairraiisooetonsn\n",
            "rassei\n",
            "ro sa\n",
            "os  \n",
            "tt oeeires\n",
            "tn ste i aree sneiann srs\n",
            "n srr sr\n",
            " \n",
            "\n",
            "s\n",
            "se\n",
            "snaen\n",
            "tionooee   r   e  ts aee\n",
            "ri or aa r n n r n r\n",
            "\n",
            "iinoot\n",
            "tnost\n",
            "nar\n",
            "arn\n",
            "rs r\n",
            "rerss it etreeaerat oasrro an\n",
            "n\n",
            "e tninno\n",
            "nsnnrtsa tsttt\n",
            "a t \n",
            " \n",
            "ta  itooa \n",
            " stntn\n",
            " ssossorro i inrit rsio  nr\n",
            " r eraioa ta\n",
            " oeeor  e\n",
            "sssar e\n",
            "sasae arit\n",
            "rotrnaen srsatrn\n",
            " osrn sna  \n",
            "rrie ne\n",
            "\n",
            "e\n",
            "t   r\n",
            "n is o \n",
            "e  iensta triae\n",
            " roiasenrnsit\n",
            "\n",
            "esote n \n",
            "neet totniir etr eir  \n",
            "oooosaareoo   reiennorn  nn osoro\n",
            "etnnto   s ni\n",
            "sotoseii\n",
            "rtea te\n",
            "  aoa aataraie\n",
            "sera  r \n",
            "in  a \n",
            "anoonr e tiostei\n",
            "ot\n",
            " oas\n",
            "osnaenteiesieotra iees rssere\n",
            "ae\n",
            "erait \n",
            "\n",
            "rt se\n",
            "eeiorrins\n",
            "eteio\n",
            "t r\n",
            "taar aieoion\n",
            "ora\n",
            "ear ot ono \n",
            "oattria\n",
            "easa  ss at  aasr\n",
            "ite\n",
            "renooa aoe\n",
            " as to\n",
            "oteitiaeaer rtnreataase atiir aatiaiai\n",
            "e ste atsrot\n",
            "aaisi\n",
            "n ei et \n",
            "s o atos  aet oeean isn issaoin a\n",
            "eitre\n",
            "n  sas  \n",
            "orroa aeant ii\n",
            "eanrs inanstinstntrnr s\n",
            "ea tisi\n",
            "oee\n",
            "oai\n",
            "ersitsn e  ones naar enr i aa o iaeaaisireotr r\n",
            "aa\n",
            "onttoe\n",
            "irsna\n",
            " snoe tssr\n",
            "inaeo en\n",
            "eaatatnei\n",
            "Epoch: 1/2... Step: 100... Loss: 3.1867... Val Loss: 3.2555\n",
            "Deartanhn h\n",
            "\n",
            "rohtoot\n",
            "nh\n",
            "\n",
            "sase r aaa ee ehh ttht\n",
            "ah e  h oa\n",
            "trrntssrnn\n",
            " ttnsrennotaorte  er\n",
            "eennh ea ttee  ts\n",
            "enhrhhrnn n stenooarehsorthsoethesthae str\n",
            "h t  oaon \n",
            "sos on  osoons t rsnr a\n",
            "  eo\n",
            "as rrsatso\n",
            "snenrn\n",
            "toetosr  r etotnasat hr rrrsrns sannao  enoetnoa\n",
            "enao\n",
            "etsanohrattaonhaooro\n",
            "trornosssoes\n",
            "oshta t\n",
            "ntehtototn ehso hh\n",
            "r  aenahearh rheehnarss  en eerosat\n",
            " ohehste\n",
            "reas\n",
            "hhrahoeh nre s tsaaon\n",
            "rheaen erhs\n",
            "aato o  eose ot soooese eetr treon arstaohhtoonahonshen oonnnrsnaa hes shs\n",
            "e enr\n",
            " rsseernaaaannhre ne\n",
            "anetr\n",
            " \n",
            "ananer eoosh s\n",
            "\n",
            "ehotenrn\n",
            " anoorrnne\n",
            " tsootaorn hn eaeenonh os\n",
            "hossrhe\n",
            "shna\n",
            "aasnhr ase t osrohrttssse osnane ehohrtratrnen  hr  roos oosn o\n",
            "\n",
            "ehrta r\n",
            "nrhtott  r nntos\n",
            " rttn nor soonnt\n",
            "hhnhontr\n",
            " enh aoaothnanra  s rtesrehhoanea\n",
            "o\n",
            "stt nonntr\n",
            "oan  s  h e asahraersrt n a sennrs ho\n",
            "ho\n",
            "ehnhornors ho e as ahaoetsset aae o\n",
            "eearerosesnha nsaoo e   oet h hae\n",
            "nsansr oer\n",
            "aaee\n",
            "a\n",
            "s\n",
            "tenrnerarent  trtre\n",
            " r  enoanos\n",
            "ohe toah ntathnssoh roh a  hrt s\n",
            "\n",
            "aeh a nhnohthhna\n",
            "ho  r hoa sronheooeea sn r  so\n",
            "ts\n",
            "\n",
            "Epoch: 1/2... Step: 150... Loss: 3.1943... Val Loss: 3.1369\n",
            "Dearrto oa orahror oo rte  \n",
            "hha o h\n",
            " ha oeoa eannnoorn ho ooeooetse  t easeotasoae oo t tasr a hoerh sohon oha nnnon nr tnrhsn\n",
            " e hha\n",
            "osrr ohoea t he a  ets hhah ne\n",
            " o tass \n",
            "nethresoeatt\n",
            "osten entoreeoor\n",
            "oon aor\n",
            "nrns os \n",
            "nn\n",
            " o\n",
            "rso s  or raen hs a\n",
            "rst rnra\n",
            "arhensrs \n",
            " oe ohnsaoot ae ro\n",
            "taae\n",
            "s\n",
            "nrs \n",
            "r roh t oeao et\n",
            "rsaehoo rohao oreo  srr \n",
            "orsostneta\n",
            "h\n",
            "tro reear aroset eato\n",
            "e\n",
            "ntn tht \n",
            "onr\n",
            "o eaaeostht het rta n aoa \n",
            "nr or aarorehh  \n",
            "tthnteo o \n",
            "h arehe esrheetnoossn\n",
            "\n",
            "t saaarh or tenetaar\n",
            "oa tt\n",
            "a eae hnh nheoeeoe \n",
            "tasr ro  natst\n",
            "ee  oe  \n",
            " t hhonohhonrn eoar tasheteootooeho  s\n",
            "tato  ah o  rer\n",
            " aoroah\n",
            "\n",
            "sse h  t s\n",
            "a\n",
            "\n",
            " araatsn\n",
            "h \n",
            "n hho\n",
            "or hannrnshrtarn  nnhe\n",
            "htnn\n",
            "aeehhe\n",
            "n\n",
            " orhsrtrrasr neo r\n",
            " \n",
            "rrre hr n\n",
            "oettrsr e\n",
            "\n",
            "o \n",
            "r te a \n",
            " arareoo\n",
            " h sh\n",
            "hats\n",
            "s\n",
            "sohhea raaa\n",
            "\n",
            "sas nhnehtons s shrsoashe   are s\n",
            "aosatrn \n",
            " hh hrrn\n",
            "hnehe ho th  a\n",
            "reotthso\n",
            "e  tsrneo oeotsoh  o hnatsn osaee \n",
            "a  atnharaa\n",
            "sto\n",
            "h   hh s  hrentr tneth set e e\n",
            "so oth teee rasae \n",
            "eonsro  athstaranroro\n",
            "rrhoa  \n",
            "htoo  htora\n",
            "ea\n",
            " r\n",
            " sto  n s neaaao atooe\n",
            "Epoch: 2/2... Step: 200... Loss: 2.7930... Val Loss: 2.8440\n",
            "Deard ha ri uater tisntn s oo ohe tianrettoe heredessnentsai ot m naddnhileee, mhaid a itsroersat ntlao,t\n",
            "Ise\n",
            "odd tatshes san ti tht oed\n",
            "otol nrsaedn nhe wetaauessan on nr tod eual, aernharsrsder airsne sh hie tnr sirrhre\n",
            "\n",
            "EIA rsdd, or,sete, eur nu oun\n",
            "e rtatotsn\n",
            "\n",
            "IA oe h sstst o aar stnt oe air\n",
            "\n",
            ":t iied otdhnuted ii so s eh sis ht hod on iun anrhinennthe wetnss onlses tars iou hhe\n",
            "\n",
            "Trsetirershate, tos\n",
            "\n",
            "I hn,rr tos,tront\n",
            "ho mee ni sn ho wust\n",
            "\n",
            "\n",
            "AIentiasrorr suslta h ho se\n",
            "hn arsss t hon ordn\n",
            " h ottn a hauun\n",
            "Itshrlthssnrrd n er e ehtoa he, sot oreld\n",
            "Aot mhre\n",
            "onr thod hi t a iio anur tte, trot in tur at eir h oss\n",
            "I\n",
            "ITr ee ens\n",
            " e sietnold ard, errhn hr enls t aail aaue\n",
            "\n",
            "Thanet nsesd houto tre\n",
            "\n",
            "STshtnn attartoo e nselr ttraittho ahtse outs nosrssd ni nu ta aa athr ans\n",
            " e tr hr, ine,\n",
            "oaaaeln wen o rs nt a oe e e o he we wot o rt se aes\n",
            "A\n",
            "T i tan ws oe s et sie\n",
            "Ahesrn ir, sttot ie t t sorttr on oadts nure oit nero aiuenrn\n",
            "oor sh hon minsnts ersertse, eertrndte\n",
            "Ir\n",
            "ho aa mhr he aerht tets wrold\n",
            " uie\n",
            "Epoch: 2/2... Step: 250... Loss: 2.6296... Val Loss: 2.5938\n",
            "Dear sy mh ware an\n",
            "Ar\n",
            "Sr ar in tord tished, tetals aesennsod.A\n",
            "Tyrandshy tasel sends ao teunoton,\n",
            "I, mi m ot hysethr, an te th fhls wol had.\n",
            "\n",
            "LEIdsd mu od or aot\n",
            "Bi w oe te shad med bidd fht ot\n",
            "atoltest wa aushs,\n",
            "Arsr bedt fheson\n",
            "Tre sldod,rnnntsol aulid\n",
            "\n",
            "CLAE weualou wime tysie br hu is mon wore mets feastaulonsan hnrhel wumst assth net hhd.Tas hhslad hes fanns, banren, anden, ilssht or tenil hhtes wy mr bes,\n",
            "torethhase itia sere bile b faratoe biunetheet, ae,s weuane tar botderon ae bhin wrllrelterhatorrthteth my wy by ad whlst n turh io ie my iulan bot he me w instod wilees\n",
            "Id:sh nh o tremhtald fhe,\n",
            "Tunrtos inloshos haretit, mos tod\n",
            "I t se,:adot\n",
            "I: wedeler or iu hellantiae,, hlnloudhe ton ildat,srilesdrrasos tilr w mine,'ant, whos on.\n",
            "LNR::orad, au sod tat horsertan oat had.\n",
            "N: as au mi fedset s ar,.A: m nue and olle m ta be hou oote tee, shold ousds hhet wort itarsoanlsoatele h felir,\n",
            "Teedelod\n",
            "Se bed\n",
            "ariros\n",
            "tne f ar sr w mrtyrt,\n",
            "Irtetha w toe fiagess,,,rir tin, woultthanls felim, f ae,,\n",
            "Epoch: 2/2... Step: 300... Loss: 2.4923... Val Loss: 2.4721\n",
            "Dearyitins ivan tot\n",
            "Atharer hety.\n",
            "Tind fy hedeur sede suthif bim mondis burhirimasteunt bo he mathiut\n",
            "O thy\n",
            "Shed,\n",
            "Ted bod\n",
            "Sod teds, tos ito al sureum os:\n",
            "osenroud afenllt wimery\n",
            "ans terard puriulletir hy fe menere fo fhe,\n",
            "hor\n",
            "Tire sell ounoed tasiteseufhathlll on aseryrur be hhens serasheut why anstoe seuthore fyanole hy\n",
            "An\n",
            "Oidhiut\n",
            "Su oeely weerhineestaflosort my mr por avt at\n",
            "I at me ty parleteramheminderath mronoset y\n",
            "I\n",
            "Tirheulh por tasee ten as ametotort wy micored\n",
            "Trale tetes as ty ted ba purery auchas mar ant\n",
            "Id tar art, tit avedhicoror,.S\n",
            "A myethres\n",
            "A wise me hheuetafe hash neut\n",
            "\n",
            "CANArrrand as mores il athate,\n",
            "Thet if ave wh srlit\n",
            "O fhllalotos\n",
            "Ioe,\n",
            "anean te mhessirhyoshed hin orease, s oorarle by,\n",
            "Ard tirenon\n",
            "Chon har,of sil br panrsoun iv feld whon hende wot,, be t wy al fe hon, shol made or frasond tinoe on sithinte memhoud\n",
            "Cin bf hironla forhe taueeseelhimorhhr fiu melos bo t fe ar asse heus mirhed\n",
            "Arse tes t thrin banlelh sose fo bauthe hold oo botheeramall, tors isd hir meslesent\n",
            "Epoch: 2/2... Step: 350... Loss: 2.3698... Val Loss: 2.4131\n",
            "Dear ou tan ar siw wer shild,\n",
            "Te sy as\n",
            "He, hy bhals wougsaraun ose, sy brisor band,\n",
            "Irof tiul he hal win al ar, ond ag, ton in sor,\n",
            "CELE ot\n",
            "Aumy.\n",
            "Sy:, il ave ar my the hithroles ir be soun mathor me borangid so bon as\n",
            "Oore memilasif as on he afdod is, hrot,.\n",
            "\n",
            "NAE\n",
            "RR\n",
            "Adisirriwhot s fuu th tit morethur shanlt,\n",
            "Se maviricad womh,\n",
            "Arssil, be w wor frered mite thon hor\n",
            "Ty ounon,\n",
            "What wetane old, we fyan,::eriwirh thind misile o hus,\n",
            "I'sesold\n",
            "Suthor:\n",
            "ha henhe comhim amath ate mrre mo murt,\n",
            "Tr i arir in by har mr oret:.\n",
            "Bndad whils micy fyim an bur wamh an\n",
            "hou,\n",
            "Mod,\n",
            "I oules af file tur fy bot ivesar, st on solan thrthimess whas.S iron mame monher thes irersisestetin tich if ovand\n",
            "Ta ofime\n",
            "Sat, ar or hit the toraco srin win sos blot wedh,\n",
            "Mot ban hrinthre fhande audery\n",
            "Tunis.\n",
            "Bne thit ans thrramd and,\n",
            "I wy fores\n",
            "He in as ithod al medare il,\n",
            "Mad aurarane,\n",
            "h bich afe besius indichrenton sosheugou honde an fis an.\n",
            "\n",
            "OA mas weuros fam atorh tes,\n",
            "Br toss a his ar hy ons we, be fre,: he fit fod t tou hod \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnI-lDmFFcpd"
      },
      "source": [
        "##### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5g6o_-6aMA0",
        "outputId": "596df67c-b963-45b1-c5ee-4eb0ed4ac90d"
      },
      "source": [
        "print(sample(vanilla_model, 100, prime='The', top_k=10))"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Then have by my liget\n",
            "To stop the plory:\n",
            "But see, the what you, thou loyous my with our should so lord; \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7WSq-vXZPJW",
        "outputId": "e13d86d7-48c9-492b-d41e-2a4bd4877627"
      },
      "source": [
        "print(sample(lstm_model, 100, prime='The', top_k=10))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thessaf sur.\n",
            "Alane braid bringram.\n",
            "Time, thotels of yourdlace potim the condew a kind was\n",
            "A bran sore si\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDTqebfHaQsg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}