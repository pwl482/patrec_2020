{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3cWwlbatGdm"
   },
   "source": [
    "# Mustererkennung/Machine Learning - Assignment 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:46:51.443420Z",
     "start_time": "2018-12-11T12:46:50.570749Z"
    },
    "id": "luPsF5SptGdt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from statistics import mean\n",
    "import random\n",
    "from numpy import linalg as LA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://vitalflux.com/convert-sklearn-dataset-pandas-dataframe/\n",
    "\n",
    "# Load the IRIS dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "names = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WaByN8YtGdx"
   },
   "source": [
    "#### Splitting the data into training/test and according to their class memberships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None, stratify=y)\n",
    "\n",
    "X_train_setosa = X_train[np.where(y_train==0)]\n",
    "X_train_versicolor = X_train[np.where(y_train==1)]\n",
    "X_train_virginica = X_train[np.where(y_train==2)]\n",
    "X_train_versicolor_virginica = np.concatenate((X_train_versicolor,X_train_virginica))\n",
    "\n",
    "X_test_setosa_v_v = X_test[np.where(y_test==0)]\n",
    "X_test_versicolor_virginica = X_test[np.where(y_test!=0)]\n",
    "y_test_versicolor_virginica = y_test[np.where(y_test!=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Perceptron\n",
    "Implement the Perceptron algorithm using Python (incl. Numpy etc.) and use it on the Iris-dataset. Train the algorithm to seperate Setosa from Versicolour and Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:46:51.501147Z",
     "start_time": "2018-12-11T12:46:51.445163Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "CpeeFeXKtGdu",
    "outputId": "cf860553-0831-4606-e3ca-81959af4ee2c"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-239-485676432dcc>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-239-485676432dcc>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    def train(self, P_X, N_X):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def center_data(P_X, N_X):\n",
    "        # TO DO: center input data, so that the mean is zero\n",
    "    \n",
    "    def train(self, P_X, N_X):\n",
    "        # P_X and N_X need to pandas data frames\n",
    "        # Initialization:\n",
    "        # Eva: first center all data, then initialize w with avg(Setosa)\n",
    "        w = np.array(P_X.apply(mean))\n",
    "        # Looping:\n",
    "        while True:\n",
    "            # create copy of w:\n",
    "            w_updated = [x for x in w]\n",
    "            # select the dataset from which to take v:\n",
    "            per = len(P_X) / (len(P_X) + len(N_X))\n",
    "            selected_set = ['P' if random.random() <= per else 'N']\n",
    "            # select random point v:\n",
    "            if selected_set == 'P':\n",
    "                random_index = random.randint(0, len(P_X) - 1)\n",
    "                v = np.array(P_X.iloc[random_index])\n",
    "            else:\n",
    "                random_index = random.randint(0, len(N_X) - 1)\n",
    "                v = np.array(N_X.iloc[random_index])\n",
    "            # testing:\n",
    "            if selected_set == 'P' and w @ v > 0:\n",
    "                continue\n",
    "            elif selected_set == 'N' and w @ v < 0:\n",
    "                continue\n",
    "            elif selected_set == 'P' and w @ v <= 0:\n",
    "                w_updated = w_updated + v\n",
    "            elif selected_set == 'N' and w @ v >= 0:\n",
    "                w_updated = w_updated - v\n",
    "            # stop criterion:\n",
    "            if LA.norm(w - w_updated) <= self.threshold:\n",
    "                break\n",
    "        return w\n",
    "    \n",
    "    def accuracy(self, labels, predictions):\n",
    "        return np.mean(labels == predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        self.w_hat = None\n",
    "        \n",
    "    def center_data(self, P_X, N_X):\n",
    "        # subtract mean from data\n",
    "        m = np.mean(np.concatenate((P_X, N_X)), axis=0)\n",
    "        return P_X-m, N_X-m\n",
    "    \n",
    "    def train(self, P_X, N_X):\n",
    "        # center data\n",
    "        P_centered, N_centered = self.center_data(P_X, N_X)\n",
    "        # initialize w_hat\n",
    "        self.w_hat = np.average(P_centered, axis=0)\n",
    "        print('Initialize weights:', self.w_hat)\n",
    "        w = self.w_hat\n",
    "        while np.linalg.norm(self.w_hat - w) <= self.threshold:\n",
    "            w = self.w_hat\n",
    "            # select random vector v in NUP\n",
    "            # randomly choose if taking from N or from P\n",
    "            tmp = np.random.choice([0,1])\n",
    "            # if v in P\n",
    "            if tmp == 1:\n",
    "                v = np.random.choice(range(P_centered.shape[0]))\n",
    "                v = P_centered[v,:]\n",
    "                # if projection of v onto w is positive, continue,\n",
    "                # else update w_hat with v\n",
    "                if w @ v <= 0:\n",
    "                    self.w_hat = w + v\n",
    "            # if v in N\n",
    "            else:\n",
    "                v = np.random.choice(range(N_centered.shape[0]))\n",
    "                v = N_centered[v,:]                \n",
    "                # if projection of v onto w is negative, continue,\n",
    "                # else update w_hat with v\n",
    "                if w @ v >= 0:\n",
    "                    self.w_hat = w - v\n",
    "        print('Final weights:', self.w_hat)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # center data\n",
    "        m = np.mean(X, axis=0)\n",
    "        X_centered = X-m\n",
    "        Y_pred = X@self.w_hat\n",
    "        # return: positive class == positive value == 1\n",
    "        #         negative class == negative value == -1\n",
    "        Y_pred[np.where(Y_pred<0)] = -1\n",
    "        Y_pred[np.where(Y_pred>=0)] = 1\n",
    "        return Y_pred\n",
    "    \n",
    "    \n",
    "    def accuracy(self, positive_label, labels, predictions):\n",
    "        labels[np.where(labels==positive_label)] = 1\n",
    "        labels[np.where(labels!=positive_label)] = -1\n",
    "        return np.sum(labels == predictions)/predictions.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train single perceptron to distinguish between setosa vs versucolour and virginica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "Single_Perceptron = Perceptron(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize weights: [-0.84833333  0.345      -2.2925     -0.95166667]\n",
      "Final weights: [-0.52    0.9975 -2.345  -0.855 ]\n"
     ]
    }
   ],
   "source": [
    "Single_Perceptron.train(X_train_setosa, X_train_versicolor_virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Single_Perceptron.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',Single_Perceptron.accuracy(0, y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) What happens if you use the algorithm to seperate Versicolour from Virginica? (Evaluate multiple runs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_versicolour_vs_virginica = Single_Perceptron.predict(X_test_versicolor_virginica)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',Single_Perceptron.accuracy(1,y_test_versicolor_virginica, y_pred_versicolour_vs_virginica))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  1]), array([10, 10]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how many items of each class the test set contains\n",
    "np.unique(y_test_versicolor_virginica, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.]), array([20]))"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how many itesm for each predicted class exist in the output labels\n",
    "np.unique(y_pred_versicolour_vs_virginica, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "Applying a perceptron classifier trained to separate setosa flowers from virginica and versicolour to predict species for an unlabelled set of only versicolours and virginicas leads to a classification performance equal to random guessing. The orthogonal vector w was trained to produce negative projections for versicolour and virginica which it actually did as shown by the label count in the returned prediction array. Yet, because we forced the perceptron evaluation to consider a class as positive that it naturally considers to be negative leads to poor evaluation results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find a way to solve the problem and obtain the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2: Multilayer-Perceptron (MLP)\n",
    "Implement a class that builds an MLP with both variable depth D (number of layers) and variable number of neurons ni for each layer i = 1,...,D. Produce outputs on the ZIP- Dataset2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Perceptron.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
