{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from sklearn.datasets import load_iris\n",
    "#from statistics import mean\n",
    "# import random\n",
    "#from numpy import linalg as LA\n",
    "#from sklearn import decomposition\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 1. Backpropagation\n",
    "Add Backpropagation to your MLP and train the model on the ZIP-Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, threshold, depth, layer_width, gamma=0.01, iterations=1000):\n",
    "        \"\"\"\n",
    "        This constructor sets random network weights and checks if the input depth matches the provided layers.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma # learning rate\n",
    "        self.iterations = iterations\n",
    "        self.threshold = threshold\n",
    "        if depth < 2:\n",
    "            raise Exception(\"Depth needs to be at least 2 (one hidden and one output layer)!!\")  \n",
    "        self.depth = depth\n",
    "        if not len(layer_width) == (depth + 1):\n",
    "            raise Exception(\"'layer_width' needs to be of length 'depth' + 1\")  \n",
    "        self.layer_width = layer_width\n",
    "        self.network_weights = []\n",
    "        #self.network_biases = []\n",
    "        width_prev = self.layer_width[0]\n",
    "        for width in self.layer_width[1:]:\n",
    "            #added +1 to incorporate bias into weight\n",
    "            self.network_weights.append(np.random.randn(width_prev+1, width)* np.sqrt(1. / width_prev+1))\n",
    "            #self.network_biases.append(np.zeros((1, width)))\n",
    "            width_prev = width\n",
    "    \n",
    "    def heaviside(self, Z):\n",
    "        \"\"\"This Function is a tiny implementation of the heaviside step function.\"\"\"\n",
    "        return (Z > 0).astype(int)\n",
    "    \n",
    "    def heaviside_derivative(self, Z):\n",
    "        \"\"\"This Function returns the evaluated derivative of the heaviside step function.\"\"\"\n",
    "        return np.where(Z == 0, 0, np.inf)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1./(1.+np.exp(-Z))\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        return self.sigmoid(Z) * (1-self.sigmoid(Z))\n",
    "    \n",
    "    def error_function(self, y_i, t_i):\n",
    "        return 0.5 * np.power(y_i - t_i, 2)\n",
    "    \n",
    "    def error_function_derivative(self, y_i, t_i):\n",
    "        return y_i - t_i\n",
    "    \n",
    "    def forward_pass(self, X, f_activation, deriv_activation):\n",
    "        \"\"\"This Function passes the input X through all weights and returns the prediction vector.\"\"\"\n",
    "        self.activated_X_i = []\n",
    "        self.derivated_X_i = []\n",
    "        X_i = X.copy()\n",
    "        for i in range(self.depth):\n",
    "            X_i = np.hstack((X_i, np.ones((X_i.shape[0],1))))\n",
    "            z_i = X_i @ self.network_weights[i]\n",
    "            X_i = f_activation(z_i)\n",
    "            deriv_X_i = deriv_activation(z_i)\n",
    "            self.activated_X_i.append(X_i)\n",
    "            self.derivated_X_i.append(np.diag(np.mean(deriv_X_i, axis=0)))\n",
    "        return X_i\n",
    "    \n",
    "    def backward_pass_updates(self, X, deriv_error):\n",
    "        \"\"\"This Function passes backwards through the network for backpropagation.\"\"\"\n",
    "        delta = deriv_error @ self.derivated_X_i[self.depth-1]\n",
    "        o_hat = self.activated_X_i[self.depth-2]\n",
    "        o_hat = np.hstack((o_hat, np.ones((o_hat.shape[0],1))))\n",
    "        d_W_i = - self.gamma * o_hat.T @ delta\n",
    "        self.network_weights[self.depth-1] += d_W_i\n",
    "        for i in range(self.depth-2, -1, -1):\n",
    "            delta = delta @ self.network_weights[i+1][:-1,].T @ self.derivated_X_i[i]\n",
    "            if i == 0:\n",
    "                o_hat = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "                d_W_i = - self.gamma * o_hat.T @ delta\n",
    "            else:\n",
    "                o_hat = self.activated_X_i[i-1]\n",
    "                o_hat = np.hstack((o_hat, np.ones((o_hat.shape[0],1))))\n",
    "                d_W_i = - self.gamma * o_hat.T @ delta\n",
    "            self.network_weights[i] += d_W_i\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        y_true = y_train.reshape((y_train.shape[0],1))\n",
    "        for i in range(self.iterations):\n",
    "            y_i = self.forward_pass(X_train, self.sigmoid, self.sigmoid_derivative)\n",
    "            error = self.error_function(y_i, y_true)\n",
    "            deriv_error = self.error_function_derivative(y_i, y_true)\n",
    "            self.backward_pass_updates(X_train, deriv_error)\n",
    "    \n",
    "    def predict(self, X, cutoff=0.5):\n",
    "        \"\"\"This function passes the input X to the iteration function.\"\"\"\n",
    "        #X_i = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "        return ((self.forward_pass(X, self.sigmoid, self.sigmoid_derivative) > cutoff).astype(int)).ravel()\n",
    "    \n",
    "    def accuracy(self, labels, predictions):\n",
    "        \"\"\"This function calculates the binary class accuracy for given true/predicted labels.\"\"\"\n",
    "        return np.mean(labels == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Zip Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '/Users/Eva/Downloads/zip.train'\n",
    "path_to_test = '/Users/Eva/Downloads/zip.test'\n",
    "training_data = np.array(pd.read_csv(path_to_train, sep=' ', header=None))\n",
    "test_data = np.array(pd.read_csv(path_to_test, sep =' ',header=None))\n",
    "\n",
    "X_train_zip, y_train_zip = training_data[:,1:-1], training_data[:,0]\n",
    "X_test_zip, y_test_zip = test_data[:,1:], test_data[:,0]\n",
    "\n",
    "X_train_zip = X_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "y_train_zip = y_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "\n",
    "X_test_zip = X_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]\n",
    "y_test_zip = y_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = MLP(threshold=0.01, depth=3, layer_width=[X_train_zip.shape[1], 100, 10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_v in mlp_network.network_weights:\n",
    "    print(weight_v.shape)\n",
    "# print(mlp_network.network_biases[0].shape) <- moved into weight matrix\n",
    "# print(mlp_network.network_biases[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network.train(X_train_zip, y_train_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp_network.predict(X_test_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_network.accuracy(y_test_zip, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_mlp, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_zip, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a mean accuracy over multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list_mlp = []\n",
    "n_runs = 10\n",
    "for i in range(n_runs):\n",
    "    mlp_network = MLP(threshold=0.01, depth=3, layer_width=[X_train_zip.shape[1], 100, 10, 1], iterations=100)\n",
    "    mlp_network.train(X_train_zip, y_train_zip)\n",
    "    y_pred_loop = mlp_network.predict(X_test_zip)\n",
    "    acc_list_mlp.append(mlp_network.accuracy(y_test_zip, y_pred_loop))\n",
    "print(\"Mean Acc over\", n_runs, \"runs, with random weights is:\", np.mean(acc_list_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result:\n",
    "Works nicely :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Optimize width (the number of neurons in a hidden layer; it is usually the same for all of them) and depth of the network. Try to find a setting that trains in a reasonable time. Plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X_train, y_train, y_test, widths, depths, nr_folds = 5):\n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "    fold_indices_list = np.array_split(indices, nr_folds)\n",
    "    \n",
    "    depth_accs = []\n",
    "    for depth in depths:\n",
    "        width_accs = []\n",
    "        for width in widths:\n",
    "            print(\"Cross Val depth:\",depth,\"width:\",width)\n",
    "            cval_acc_mlp = []\n",
    "            for fold_counter, val_indices in enumerate(fold_indices_list):\n",
    "                X_val = X_train[val_indices]\n",
    "                y_val = y_train[val_indices]\n",
    "                X_training = np.delete(X_train, val_indices, 0)\n",
    "                y_training = np.delete(y_train, val_indices, 0)\n",
    "                width_list = [X_train_zip.shape[1]] + [width] * (depth-1) + [1]\n",
    "                cross_val_mlp = MLP(threshold=0.001, depth=depth, layer_width=width_list, iterations=500)\n",
    "                cross_val_mlp.train(X_training, y_training)\n",
    "                y_pred_cv = mlp_network.predict(X_val)\n",
    "                cval_acc_mlp.append(mlp_network.accuracy(y_val, y_pred_cv))\n",
    "            width_accs.append(cval_acc_mlp)\n",
    "        depth_accs.append(width_accs)\n",
    "    return(np.array(depth_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_widths = [2, 5, 10, 20]\n",
    "cv_depths = [2, 3, 5, 10]\n",
    "cv_accuracies = cross_validation(X_train_zip, y_train_zip, y_test_zip, cv_widths, cv_depths, nr_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Show some digits that are classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network_refined = MLP(threshold=0.01, depth=3, layer_width=[X_train_zip.shape[1], 100, 10, 1])\n",
    "\n",
    "for weight_v in mlp_network_refined.network_weights:\n",
    "    print(weight_v.shape)\n",
    "\n",
    "mlp_network_refined.train(X_train_zip, y_train_zip)\n",
    "\n",
    "y_pred_refined = mlp_network_refined.predict(X_test_zip)\n",
    "\n",
    "mlp_network_refined.accuracy(y_test_zip, y_pred_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_few_numbers(Xi):\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    i = 1\n",
    "    for X in Xi:\n",
    "        ax = plt.subplot(len(Xi)//10 + 1, 10, i)\n",
    "        plt.imshow(1-X.reshape((16, 16)), cmap='gray')\n",
    "        plt.axis('off')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the missclassified numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_few_numbers(X_test_zip[np.where(y_test_zip != y_pred_refined)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Plot your first weight layer as a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_vector_img in mlp_network_refined.network_weights:\n",
    "    plt.imshow(weight_vector_img, cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
