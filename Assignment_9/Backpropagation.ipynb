{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Add Backpropagation to your MLP and train the model on the ZIP-Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialization**\n",
    "- define threshold for activation function (UNUSED), number of layers, number of neurons per layer\n",
    "- for each layer, initialize a weight matrix with random numbers (dim = #neurons in previous leyer x #neurons in current layer) and a bias vector with ones\n",
    "\n",
    "**2. Training**\n",
    "- ***Feedforward***\n",
    "    - Feed batch data through layers (f_i(w_i*x+b), using weight matrices w and activation functions f)\n",
    "    - for every neuron/layer, store output value y_i and derivative y*_i\n",
    "    \n",
    "- ***Backpropagation*** \n",
    "    - Quantify network error (MSE = 1/2 * (y_n-t)^2)\n",
    "    - Update weights w_i in layer i by product of backpropagated error, derivative, input vector and learning rate (SGD)\n",
    "    \n",
    "**3. Prediction/Inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo: introduce bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, depth, layer_width, threshold, learning_rate):\n",
    "        \"\"\"\n",
    "        This constructor sets random network weights and checks if the input depth matches the provided layers.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = depth\n",
    "        if not len(layer_width) == (depth + 1):\n",
    "            raise Exception(\"'layer_width' needs to be of length 'depth' + 1\")  \n",
    "        self.layer_width = layer_width\n",
    "        self.network_weights = [] # store layer weights\n",
    "        self.network_derivatives = [] # store derivatives of activated layer neurons\n",
    "        self.network_outputs = [np.zeros((1,self.layer_width[0]))] # store output values of activated layer neurons\n",
    "        width_prev = self.layer_width[0]\n",
    "        for width in self.layer_width[1:]:\n",
    "            # add 1 dimension to weight matrix to account for bias\n",
    "            self.network_weights.append(np.random.randn(width_prev, width)* np.sqrt(1. / (width_prev)))\n",
    "            self.network_derivatives.append(np.zeros((1, width))) \n",
    "            self.network_outputs.append(np.zeros((1, width)))\n",
    "            width_prev = width\n",
    "        self.error_memory = []\n",
    "        \n",
    "\n",
    "    ############  Activation functions  ######### \n",
    "    def heaviside(self, X):\n",
    "        \"\"\"This Function is a tiny implementation of the heaviside step function.\"\"\"\n",
    "        return (X >= self.threshold).astype(int)\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        sig = 1/(1+np.exp(-X))\n",
    "        return (X >= self.threshold).astype(int)\n",
    "    #############################################\n",
    "        \n",
    "    def mean_squared_error(self, Y_m, T_m):\n",
    "        \"\"\"Quantify rrror after feedforward step.\"\"\"\n",
    "        return 1/2 * np.power((Y_m - T_m),2)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"This Function passes the input X through all weights and returns the prediction vector.\"\"\"\n",
    "        X_i = X.copy()\n",
    "        self.network_outputs[0] = X_i\n",
    "        for i in range(self.depth):\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i] \n",
    "            # Apply activation function\n",
    "            X_i = self.sigmoid(z_i)\n",
    "            # Store derivatives\n",
    "            D_i = X_i*(1-X_i)\n",
    "            self.network_outputs[i+1] = X_i\n",
    "            # see Paul's implementation for diagonal derivative matrices\n",
    "            self.network_derivatives[i] = D_i\n",
    "        return X_i\n",
    "    \n",
    "    def backpropagate(self, error):\n",
    "        \"\"\"Backpropagate error and update weight matrices.\"\"\"\n",
    "        d_last_hidden_layer = self.network_derivatives[self.depth-1] * error\n",
    "        for i in range(self.depth-1):\n",
    "            d_tmp = d_last_hidden_layer\n",
    "            for j in range(i, self.depth-1):\n",
    "                d_tmp = self.network_derivatives[j]*self.network_weights[j+1].T\n",
    "            d_tmp *= d_tmp\n",
    "            dW = -self.learning_rate * self.network_outputs[i].T @ d_tmp \n",
    "            self.network_weights[i] += dW\n",
    "            # TODO: also update biases?\n",
    "        # update last layer weights\n",
    "        d_W_last_hidden_layer = -self.learning_rate * self.network_outputs[self.depth-1].T @ d_last_hidden_layer \n",
    "        self.network_weights[self.depth-1] += d_W_last_hidden_layer\n",
    "        return\n",
    "    \n",
    "    def train(self, X, Y, M):\n",
    "        \"\"\"Train the MLP on (X,Y) in M equally sized batches using feedforward and backpropagation with Stochastic Gradient Descent.\"\"\"\n",
    "        # Shuffle data indices and split in subsets of size M\n",
    "        batch_indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(batch_indices)\n",
    "        batch_splits = np.array_split(batch_indices, M)\n",
    "        for m in range(len(batch_splits)):\n",
    "            # fetch batch\n",
    "            print('Fetch batch no.',m)\n",
    "            X_m = X[batch_splits[m]]\n",
    "            T_m = Y[batch_splits[m]]\n",
    "            Y_m = self.feed_forward(X_m)\n",
    "            # Quantify error\n",
    "            E = self.mean_squared_error(Y_m.ravel(), T_m)\n",
    "            self.error_memory.append(E)\n",
    "            dE = (Y_m.T - T_m).T\n",
    "            # Backpropagate\n",
    "            self.backpropagate(dE)\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"This function passes the input X to the iteration function.\"\"\"\n",
    "        X_i = X.copy()\n",
    "        for i in range(self.depth):\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i]\n",
    "            # Apply activation function\n",
    "            X_i = self.sigmoid(z_i)\n",
    "        return X_i.ravel()\n",
    "    \n",
    "    def accuracy(self, labels, predictions):\n",
    "        \"\"\"This function calculates the binary class accuracy for given true/predicted labels.\"\"\"\n",
    "        return np.mean(labels == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load ZIP data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '/Users/Eva/Downloads/zip.train'\n",
    "path_to_test = '/Users/Eva/Downloads/zip.test'\n",
    "training_data = np.array(pd.read_csv(path_to_train, sep=' ', header=None))\n",
    "test_data = np.array(pd.read_csv(path_to_test, sep =' ',header=None))\n",
    "\n",
    "X_train_zip, y_train_zip = training_data[:,1:-1], training_data[:,0]\n",
    "X_test_zip, y_test_zip = test_data[:,1:], test_data[:,0]\n",
    "\n",
    "# We only want to classify two different digits. You can choose which digits you want to classify youself\n",
    "\n",
    "X_train_zip = X_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "y_train_zip = y_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "\n",
    "X_test_zip = X_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]\n",
    "y_test_zip = y_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify the Zip-Dataset with the random initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = MLP(threshold=0.01, learning_rate=0.1, depth=2, layer_width=[X_train_zip.shape[1], 10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "(256, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "Layer: 1\n",
      "(10, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(mlp_network.depth):\n",
    "    print('Layer:', i)\n",
    "    print(mlp_network.network_weights[i].shape)\n",
    "    print(mlp_network.network_outputs[i+1].shape)\n",
    "    print(mlp_network.network_derivatives[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch batch no. 0\n",
      "Fetch batch no. 1\n",
      "Fetch batch no. 2\n",
      "Fetch batch no. 3\n",
      "Fetch batch no. 4\n",
      "Fetch batch no. 5\n",
      "Fetch batch no. 6\n",
      "Fetch batch no. 7\n",
      "Fetch batch no. 8\n",
      "Fetch batch no. 9\n"
     ]
    }
   ],
   "source": [
    "mlp_network.train(X_train_zip[:100,:], y_train_zip[:100], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "(256, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "Layer: 1\n",
      "(10, 1)\n",
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(mlp_network.depth):\n",
    "    print('Layer:', i)\n",
    "    print(mlp_network.network_weights[i].shape)\n",
    "    print(mlp_network.network_outputs[i+1].shape)\n",
    "    print(mlp_network.network_derivatives[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp_network.predict(X_test_zip[:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_zip[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_network.accuracy(y_test_zip[:100], y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([79, 21]))"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred_mlp, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([1194, 1005]))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_zip, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a mean accuracy over multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc over 100 runs, with random weights is: 0.48371532514779453\n"
     ]
    }
   ],
   "source": [
    "acc_list_mlp = []\n",
    "n_runs = 100\n",
    "for i in range(n_runs):\n",
    "    mlp_network = MLP(threshold=0.01, learning_rate=0.1,depth=2, layer_width=[X_train_zip.shape[1], 10, 1])\n",
    "    y_pred_loop = mlp_network.predict(X_train_zip)\n",
    "    acc_list_mlp.append(mlp_network.accuracy(y_train_zip, y_pred_loop))\n",
    "print(\"Mean Acc over\", n_runs, \"runs, with random weights is:\", np.mean(acc_list_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Optimize width (the number of neurons in a hidden layer; it is usually the same for all of them) and depth of the network. Try to find a setting that trains in a reasonable time. Plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Show some digits that are classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Plot your first weight layer as a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
