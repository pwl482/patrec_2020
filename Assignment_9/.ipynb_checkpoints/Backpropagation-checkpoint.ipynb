{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Add Backpropagation to your MLP and train the model on the ZIP-Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from statistics import mean\n",
    "import random\n",
    "from numpy import linalg as LA\n",
    "from sklearn import decomposition\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialization**\n",
    "- define threshold for activation function (UNUSED), number of layers, number of neurons per layer\n",
    "- for each layer, initialize a weight matrix with random numbers (dim = #neurons in previous leyer x #neurons in current layer) and a bias vector with ones\n",
    "\n",
    "**2. Training**\n",
    "- ***Feedforward***\n",
    "    - Feed batch data through layers (f_i(w_i*x+b), using weight matrices w and activation functions f)\n",
    "    - for every neuron/layer, store output value y_i and derivative y*_i\n",
    "    \n",
    "- ***Backpropagation*** \n",
    "    - Quantify network error (MSE = 1/2 * (y_n-t)^2)\n",
    "    - Update weights w_i in layer i by product of backpropagated error, derivative, input vector and learning rate (SGD)\n",
    "**3. Prediction/Inference**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, depth, layer_width, threshold, learning_rate):\n",
    "        \"\"\"\n",
    "        This constructor sets random network weights and checks if the input depth matches the provided layers.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = depth\n",
    "        if not len(layer_width) == (depth + 1):\n",
    "            raise Exception(\"'layer_width' needs to be of length 'depth' + 1\")  \n",
    "        self.layer_width = layer_width\n",
    "        self.network_weights = []\n",
    "        self.network_biases = []\n",
    "        self.network_derivatives = []\n",
    "        self.network_outputs = [np.zeros((1,self.layer_width[0]))]\n",
    "        width_prev = self.layer_width[0]\n",
    "        for width in self.layer_width[1:]:\n",
    "            self.network_weights.append(np.random.randn(width_prev, width)* np.sqrt(1. / width_prev))\n",
    "            self.network_biases.append(np.zeros((1, width)))\n",
    "            self.network_derivatives.append(np.zeros((1, width))) \n",
    "            self.network_outputs.append(np.zeros((1, width)))\n",
    "            width_prev = width\n",
    "        self.error_memory = []\n",
    "        \n",
    "    def mean_squared_error(self, Y_m, T_m):\n",
    "        \"\"\"Quantify rrror after feedforward step.\"\"\"\n",
    "        return 1/2 * np.power((Y_m - T_m),2)\n",
    "    \n",
    "    def heaviside(self, X):\n",
    "        \"\"\"This Function is a tiny implementation of the heaviside step function.\"\"\"\n",
    "        return (X >= self.threshold).astype(int)\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        sig = 1/(1+np.exp(-X))\n",
    "        return (X >= self.threshold).astype(int)\n",
    "            \n",
    "    def backpropagate(self, error):\n",
    "        \"\"\"Backpropagate error and update weight matrices.\"\"\"\n",
    "        #print('Backpropagation')\n",
    "        # from first to last layer\n",
    "        d_last_hidden_layer = self.network_derivatives[self.depth-1] * error\n",
    "        for i in range(self.depth-1):\n",
    "            d_tmp = d_last_hidden_layer\n",
    "            #print('Layer',i)\n",
    "            #print('- Get delta'+str(i))\n",
    "            for j in range(i, self.depth-1):\n",
    "                #print('-- deriatives * weights.T ')\n",
    "                #print('--',self.network_derivatives[j].shape, self.network_weights[j+1].T.shape)\n",
    "                d_tmp = self.network_derivatives[j]*self.network_weights[j+1].T\n",
    "            d_tmp *= d_tmp\n",
    "            #print('- shape', d_tmp.shape)  \n",
    "            #print('- Update weights')\n",
    "            #print('-- old weights:', self.network_weights[i].shape)\n",
    "            #print('-- lr * partial derivative delta('+str(i)+') * input o('+str(i-1)+').T')\n",
    "            #print(self.learning_rate, self.network_outputs[i].T.shape, d_tmp.shape)\n",
    "            dW = -self.learning_rate * self.network_outputs[i].T @ d_tmp \n",
    "            self.network_weights[i] += dW\n",
    "            #print('-- new weights:', self.network_weights[i].shape)\n",
    "            # TODO: also update biases?\n",
    "        # update last layer weights\n",
    "        d_W_last_hidden_layer = -self.learning_rate * self.network_outputs[self.depth-1].T @ d_last_hidden_layer \n",
    "        self.network_weights[self.depth-1] += d_W_last_hidden_layer\n",
    "        return\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"This Function passes the input X through all weights and returns the prediction vector.\"\"\"\n",
    "        #print('Feedforward')\n",
    "        X_i = X.copy()\n",
    "        self.network_outputs[0] = X_i\n",
    "        for i in range(self.depth):\n",
    "            #print('- layer', i)\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i] + self.network_biases[i]\n",
    "            #print('-- weighted sum:', z_i.shape)\n",
    "            # Apply activation function\n",
    "            #X_i = self.heaviside(z_i)\n",
    "            X_i = self.sigmoid(z_i)\n",
    "            #print('-- activated:', X_i.shape)\n",
    "            # Store derivatives\n",
    "            D_i = X_i*(1-X_i)\n",
    "            self.network_outputs[i+1] = X_i\n",
    "            self.network_derivatives[i] = D_i\n",
    "        return X_i\n",
    "    \n",
    "    def train(self, X, Y, M):\n",
    "        \"\"\"Train the MLP on (X,Y) in M equally sized batches using feedforward and backpropagation with Stochastic Gradient Descent.\"\"\"\n",
    "        # Shuffle data indices and split in subsets of size M\n",
    "        batch_indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(batch_indices)\n",
    "        batch_splits = np.array_split(batch_indices, M)\n",
    "        for m in range(len(batch_splits)):\n",
    "            # fetch batch\n",
    "            print('Fetch batch no.',m)\n",
    "            X_m = X[batch_splits[m]]\n",
    "            T_m = Y[batch_splits[m]]\n",
    "            Y_m = self.feed_forward(X_m)\n",
    "            #print('shape Xm, Ym, Tm:')\n",
    "            #print(X_m.shape, Y_m.shape, T_m.T.shape)\n",
    "            # Quantify error\n",
    "            E = self.mean_squared_error(Y_m.ravel(), T_m)\n",
    "            self.error_memory.append(E)\n",
    "            dE = (Y_m.T - T_m).T\n",
    "            #print('shape dE:', dE.shape)\n",
    "            # Backpropagate\n",
    "            self.backpropagate(dE)\n",
    "        #raise Exception(\"The train function will be implemented in Assignment 9!!!\")\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"This function passes the input X to the iteration function.\"\"\"\n",
    "        X_i = X.copy()\n",
    "        for i in range(self.depth):\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i] + self.network_biases[i]\n",
    "            # Apply activation function\n",
    "            #X_i = self.heaviside(z_i)\n",
    "            X_i = self.sigmoid(z_i)\n",
    "        return X_i.ravel()\n",
    "    \n",
    "    def accuracy(self, labels, predictions):\n",
    "        \"\"\"This function calculates the binary class accuracy for given true/predicted labels.\"\"\"\n",
    "        return np.mean(labels == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load ZIP data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '/Users/Eva/Downloads/zip.train'\n",
    "path_to_test = '/Users/Eva/Downloads/zip.test'\n",
    "training_data = np.array(pd.read_csv(path_to_train, sep=' ', header=None))\n",
    "test_data = np.array(pd.read_csv(path_to_test, sep =' ',header=None))\n",
    "\n",
    "X_train_zip, y_train_zip = training_data[:,1:-1], training_data[:,0]\n",
    "X_test_zip, y_test_zip = test_data[:,1:], test_data[:,0]\n",
    "\n",
    "# We only want to classify two different digits. You can choose which digits you want to classify youself\n",
    "\n",
    "X_train_zip = X_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "y_train_zip = y_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "\n",
    "X_test_zip = X_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]\n",
    "y_test_zip = y_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify the Zip-Dataset with the random initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = MLP(threshold=0.01, learning_rate=0.1, depth=2, layer_width=[X_train_zip.shape[1], 10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "(256, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "(1, 10)\n",
      "Layer: 1\n",
      "(10, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(mlp_network.depth):\n",
    "    print('Layer:', i)\n",
    "    print(mlp_network.network_weights[i].shape)\n",
    "    print(mlp_network.network_biases[i].shape)\n",
    "    print(mlp_network.network_outputs[i+1].shape)\n",
    "    print(mlp_network.network_derivatives[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetch batch no. 0\n",
      "Fetch batch no. 1\n",
      "Fetch batch no. 2\n",
      "Fetch batch no. 3\n",
      "Fetch batch no. 4\n",
      "Fetch batch no. 5\n",
      "Fetch batch no. 6\n",
      "Fetch batch no. 7\n",
      "Fetch batch no. 8\n",
      "Fetch batch no. 9\n"
     ]
    }
   ],
   "source": [
    "mlp_network.train(X_train_zip[:100,:], y_train_zip[:100], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "(256, 10)\n",
      "(1, 10)\n",
      "(10, 10)\n",
      "(10, 10)\n",
      "Layer: 1\n",
      "(10, 1)\n",
      "(1, 1)\n",
      "(10, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(mlp_network.depth):\n",
    "    print('Layer:', i)\n",
    "    print(mlp_network.network_weights[i].shape)\n",
    "    print(mlp_network.network_biases[i].shape)\n",
    "    print(mlp_network.network_outputs[i+1].shape)\n",
    "    print(mlp_network.network_derivatives[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mlp = mlp_network.predict(X_test_zip[:100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_zip[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_network.accuracy(y_train_zip[:100], y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([79, 21]))"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred_mlp, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([1194, 1005]))"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_zip, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a mean accuracy over multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc over 100 runs, with random weights is: 0.48371532514779453\n"
     ]
    }
   ],
   "source": [
    "acc_list_mlp = []\n",
    "n_runs = 100\n",
    "for i in range(n_runs):\n",
    "    mlp_network = MLP(threshold=0.01, learning_rate=0.1,depth=2, layer_width=[X_train_zip.shape[1], 10, 1])\n",
    "    y_pred_loop = mlp_network.predict(X_train_zip)\n",
    "    acc_list_mlp.append(mlp_network.accuracy(y_train_zip, y_pred_loop))\n",
    "print(\"Mean Acc over\", n_runs, \"runs, with random weights is:\", np.mean(acc_list_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Optimize width (the number of neurons in a hidden layer; it is usually the same for all of them) and depth of the network. Try to find a setting that trains in a reasonable time. Plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Show some digits that are classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Plot your first weight layer as a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
