{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Add Backpropagation to your MLP and train the model on the ZIP-Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from statistics import mean\n",
    "import random\n",
    "from numpy import linalg as LA\n",
    "from sklearn import decomposition\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Initialization**\n",
    "- define threshold for activation function (UNUSED), number of layers, number of neurons per layer\n",
    "- for each layer, initialize a weight matrix with random numbers (dim = #neurons in previous leyer x #neurons in current layer) and a bias vector with ones\n",
    "\n",
    "**2. Training**\n",
    "- ***Feedforward***\n",
    "    - Feed batch data through layers (f_i(w_i*x+b), using weight matrices w and activation functions f)\n",
    "    - for every neuron/layer, store output value y_i and derivative y*_i\n",
    "    \n",
    "- ***Backpropagation*** \n",
    "    - Quantify network error (MSE = 1/2 * (y_n-t)^2)\n",
    "    - Update weights w_i in layer i by product of backpropagated error, derivative, input vector and learning rate (SGD)\n",
    "**3. Prediction/Inference**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, depth, layer_width, threshold, learning_rate):\n",
    "        \"\"\"\n",
    "        This constructor sets random network weights and checks if the input depth matches the provided layers.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.learning_rate = learning_rate\n",
    "        self.depth = depth\n",
    "        if not len(layer_width) == (depth + 1):\n",
    "            raise Exception(\"'layer_width' needs to be of length 'depth' + 1\")  \n",
    "        self.layer_width = layer_width\n",
    "        self.network_weights = []\n",
    "        self.network_biases = []\n",
    "        self.network_derivatives = [np.zeros((1,self.layer_width[0]))]\n",
    "        self.network_outputs = [np.zeros((1,self.layer_width[0]))]\n",
    "        width_prev = self.layer_width[0]\n",
    "        for width in self.layer_width[1:]:\n",
    "            self.network_weights.append(np.random.randn(width_prev, width)* np.sqrt(1. / width_prev))\n",
    "            self.network_biases.append(np.zeros((1, width)))\n",
    "            self.network_derivatives.append(np.zeros((1, width))) # tmp\n",
    "            self.network_outputs.append(np.zeros((1, width)))\n",
    "            width_prev = width\n",
    "        \n",
    "    def mean_squared_error(self, Y_m, T_m):\n",
    "        \"\"\"Quantify rrror after feedforward step.\"\"\"\n",
    "        return 1/2 * np.power((Y_m - T_m),2)\n",
    "    \n",
    "    def heaviside(self, X):\n",
    "        \"\"\"This Function is a tiny implementation of the heaviside step function.\"\"\"\n",
    "        return (X >= self.threshold).astype(int)\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1/1+np.exp(X)\n",
    "            \n",
    "    def backpropagate(self, Error):\n",
    "        \"\"\"Backpropagate error and update weight matrices.\"\"\"\n",
    "        # from first to last layer\n",
    "        for i in range(self.depth):\n",
    "            print('Update weights in layer',i)\n",
    "            print('derivatives * error:')\n",
    "            print(self.network_derivatives[self.depth].T.shape, Error.shape)\n",
    "            tmp = self.network_derivatives[self.depth].T*Error\n",
    "            print(tmp.shape)\n",
    "            print('get partial derivatives')\n",
    "            for j in range(self.depth-1, i-1, -1):\n",
    "                print('derivatives, weights, tmp')\n",
    "                print(self.network_derivatives[j].shape,self.network_weights[j].shape,tmp.T.shape)\n",
    "                tmp = self.network_derivatives[j]@self.network_weights[j]*tmp.T\n",
    "                print(tmp.shape)\n",
    "            print('update weights')\n",
    "            # update weights: learning_rate*derivatives*weights*...*input\n",
    "            print('lr, tmp (derivatives*weights), input')\n",
    "            print(self.learning_rate, tmp.shape, self.network_outputs[i-1].shape)\n",
    "            self.network_weights[i] = self.learning_rate * tmp * self.network_outputs[i-1] \n",
    "            # TODO: also update biases?\n",
    "        return\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"This Function passes the input X through all weights and returns the prediction vector.\"\"\"\n",
    "        X_i = X.copy()\n",
    "        self.network_outputs[0] = X_i\n",
    "        self.network_derivatives[0] = X_i\n",
    "        for i in range(self.depth):\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i] + self.network_biases[i]\n",
    "            # Apply activation function\n",
    "            #X_i = self.heaviside(z_i)\n",
    "            X_i = self.sigmoid(z_i)\n",
    "            # Store derivatives\n",
    "            D_i = X_i*(1-X_i)\n",
    "            self.network_outputs[i+1] = X_i\n",
    "            self.network_derivatives[i+1] = D_i\n",
    "        return X_i\n",
    "    \n",
    "    def train(self, X, Y, M):\n",
    "        \"\"\"Train the MLP on (X,Y) in M equally sized batches using feedforward and backpropagation with Stochastic Gradient Descent.\"\"\"\n",
    "        # Shuffle data indices and split in subsets of size M\n",
    "        batch_indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(batch_indices)\n",
    "        batch_splits = np.array_split(batch_indices, M)\n",
    "        for m in batch_splits:\n",
    "            # fetch batch\n",
    "            X_m = X[m]\n",
    "            T_m = Y[m]\n",
    "            Y_m = self.feed_forward(X_m) \n",
    "            # Quantify error\n",
    "            E = self.mean_squared_error(Y_m.ravel(), T_m)\n",
    "            # Backpropagate\n",
    "            self.backpropagate(E)\n",
    "        #raise Exception(\"The train function will be implemented in Assignment 9!!!\")\n",
    "        return \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"This function passes the input X to the iteration function.\"\"\"\n",
    "        X_i = X.copy()\n",
    "        for i in range(self.depth):\n",
    "            # Compute weighted sum\n",
    "            z_i = X_i @ self.network_weights[i] + self.network_biases[i]\n",
    "            # Apply activation function\n",
    "            #X_i = self.heaviside(z_i)\n",
    "            X_i = self.sigmoid(z_i)\n",
    "        return X_i.ravel()\n",
    "    \n",
    "    def accuracy(self, labels, predictions):\n",
    "        \"\"\"This function calculates the binary class accuracy for given true/predicted labels.\"\"\"\n",
    "        return np.mean(labels == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load ZIP data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train = '/Users/Eva/Downloads/zip.train'\n",
    "path_to_test = '/Users/Eva/Downloads/zip.test'\n",
    "training_data = np.array(pd.read_csv(path_to_train, sep=' ', header=None))\n",
    "test_data = np.array(pd.read_csv(path_to_test, sep =' ',header=None))\n",
    "\n",
    "X_train_zip, y_train_zip = training_data[:,1:-1], training_data[:,0]\n",
    "X_test_zip, y_test_zip = test_data[:,1:], test_data[:,0]\n",
    "\n",
    "# We only want to classify two different digits. You can choose which digits you want to classify youself\n",
    "\n",
    "X_train_zip = X_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "y_train_zip = y_train_zip[np.logical_or(y_train_zip == 0, y_train_zip == 1)]\n",
    "\n",
    "X_test_zip = X_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]\n",
    "y_test_zip = y_test_zip[np.logical_or(y_test_zip == 0, y_test_zip == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify the Zip-Dataset with the random initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_network = MLP(threshold=0.01, learning_rate=0.1, depth=2, layer_width=[X_train_zip.shape[1], 10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0\n",
      "(256, 10)\n",
      "(1, 10)\n",
      "(1, 256)\n",
      "(1, 256)\n",
      "Layer: 1\n",
      "(10, 1)\n",
      "(1, 1)\n",
      "(1, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "for i in range(mlp_network.depth):\n",
    "    print('Layer:', i)\n",
    "    print(mlp_network.network_weights[i].shape)\n",
    "    print(mlp_network.network_biases[i].shape)\n",
    "    print(mlp_network.network_outputs[i].shape)\n",
    "    print(mlp_network.network_derivatives[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.ones((10,10))@np.ones((10,1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update weights in layer 0\n",
      "derivatives * error:\n",
      "(1, 10) (10,)\n",
      "(1, 10)\n",
      "get partial derivatives\n",
      "derivatives, weights, tmp\n",
      "(10, 10) (10, 1) (10, 1)\n",
      "(10, 1)\n",
      "derivatives, weights, tmp\n",
      "(10, 256) (256, 10) (1, 10)\n",
      "(10, 10)\n",
      "update weights\n",
      "lr, tmp (derivatives*weights), input\n",
      "0.1 (10, 10) (10, 1)\n",
      "Update weights in layer 1\n",
      "derivatives * error:\n",
      "(1, 10) (10,)\n",
      "(1, 10)\n",
      "get partial derivatives\n",
      "derivatives, weights, tmp\n",
      "(10, 10) (10, 1) (10, 1)\n",
      "(10, 1)\n",
      "update weights\n",
      "lr, tmp (derivatives*weights), input\n",
      "0.1 (10, 1) (10, 256)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-317-6d2f230dca7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_zip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_zip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-313-1ef4b1ab095c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, M)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mX_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mT_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mY_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;31m# Quantify error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-313-1ef4b1ab095c>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# Compute weighted sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mz_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_i\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# Apply activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m#X_i = self.heaviside(z_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 256)"
     ]
    }
   ],
   "source": [
    "mlp_network.train(X_train_zip[:100,:], y_train_zip[:100], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-216-1574e1e47e30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_mlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-212-519c9a0fa51e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# Compute weighted sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mz_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_i\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_biases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;31m# Apply activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m#X_i = self.heaviside(z_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 10 is different from 256)"
     ]
    }
   ],
   "source": [
    "y_pred_mlp = mlp_network.predict(X_train_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34015461573442474"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_network.accuracy(y_train_zip, y_pred_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([ 637, 1562]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred_mlp, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([1194, 1005]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_zip, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a mean accuracy over multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc over 100 runs, with random weights is: 0.49954524783992726\n"
     ]
    }
   ],
   "source": [
    "acc_list_mlp = []\n",
    "n_runs = 100\n",
    "for i in range(n_runs):\n",
    "    mlp_network = MLP(threshold=0.01, depth=2, layer_width=[X_train_zip.shape[1], 10, 1])\n",
    "    y_pred_loop = mlp_network.predict(X_train_zip)\n",
    "    acc_list_mlp.append(mlp_network.accuracy(y_train_zip, y_pred_loop))\n",
    "print(\"Mean Acc over\", n_runs, \"runs, with random weights is:\", np.mean(acc_list_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Optimize width (the number of neurons in a hidden layer; it is usually the same for all of them) and depth of the network. Try to find a setting that trains in a reasonable time. Plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Show some digits that are classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Plot your first weight layer as a grayscale image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
